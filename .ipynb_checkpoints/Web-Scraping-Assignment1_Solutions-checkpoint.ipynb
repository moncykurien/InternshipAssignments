{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB SCRAPING ASSIGNMENT 1_SOLUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. Write a python program to display all the header tags from ‘en.wikipedia.org/wiki/Main_Page’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1 class=\"firstHeading\" id=\"firstHeading\" lang=\"en\">Main Page</h1>,\n",
       " <h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2>,\n",
       " <h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2>,\n",
       " <h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2>,\n",
       " <h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2>,\n",
       " <h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2>,\n",
       " <h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2>,\n",
       " <h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2>,\n",
       " <h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2>,\n",
       " <h2>Navigation menu</h2>,\n",
       " <h3 id=\"p-personal-label\">\n",
       " <span>Personal tools</span>\n",
       " </h3>,\n",
       " <h3 id=\"p-namespaces-label\">\n",
       " <span>Namespaces</span>\n",
       " </h3>,\n",
       " <h3 id=\"p-variants-label\">\n",
       " <span>Variants</span>\n",
       " </h3>,\n",
       " <h3 id=\"p-views-label\">\n",
       " <span>Views</span>\n",
       " </h3>,\n",
       " <h3 id=\"p-cactions-label\">\n",
       " <span>More</span>\n",
       " </h3>,\n",
       " <h3>\n",
       " <label for=\"searchInput\">Search</label>\n",
       " </h3>,\n",
       " <h3 id=\"p-navigation-label\">\n",
       " <span>Navigation</span>\n",
       " </h3>,\n",
       " <h3 id=\"p-interaction-label\">\n",
       " <span>Contribute</span>\n",
       " </h3>,\n",
       " <h3 id=\"p-tb-label\">\n",
       " <span>Tools</span>\n",
       " </h3>,\n",
       " <h3 id=\"p-coll-print_export-label\">\n",
       " <span>Print/export</span>\n",
       " </h3>,\n",
       " <h3 id=\"p-wikibase-otherprojects-label\">\n",
       " <span>In other projects</span>\n",
       " </h3>,\n",
       " <h3 id=\"p-lang-label\">\n",
       " <span>Languages</span>\n",
       " </h3>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function Definition\n",
    "def header_tags(url):\n",
    "    a = requests.get(url)\n",
    "    src = a.content\n",
    "    soup = BeautifulSoup(src,'html.parser')\n",
    "    return soup.find_all([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\"])\n",
    "\n",
    "# Calling Function\n",
    "header_tags('https://en.wikipedia.org/wiki/Main_Page')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2. Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. Name, IMDB rating, Year of release) and save it in form of a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       names   years  ratings\n",
      "0   The Shawshank Redemption  (1994)      9.2\n",
      "1              The Godfather  (1972)      9.1\n",
      "2     The Godfather: Part II  (1974)      9.0\n",
      "3            The Dark Knight  (2008)      9.0\n",
      "4               12 Angry Men  (1957)      8.9\n",
      "..                       ...     ...      ...\n",
      "95                    Jagten  (2012)      8.3\n",
      "96              Citizen Kane  (1941)      8.3\n",
      "97                      1917  (2019)      8.3\n",
      "98         Full Metal Jacket  (1987)      8.2\n",
      "99       Ladri di biciclette  (1948)      8.2\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function Definition\n",
    "def IMDB_top_100(url):\n",
    "    r=requests.get(url)\n",
    "    soup=BeautifulSoup(r.content,'html.parser')\n",
    "    name=[]\n",
    "    year=[]\n",
    "    rating=[]\n",
    "    for i in soup.find_all('img'):\n",
    "        name.append(i.attrs['alt'])\n",
    "    for i in list(soup.find_all('span',attrs={\"secondaryInfo\"})):\n",
    "        year.append(i.text)\n",
    "    for i in soup.find_all('strong'):\n",
    "        rating.append(float(i.text))\n",
    "    df=pd.DataFrame({'names':name[:100],\n",
    "                     'years':year[:100],\n",
    "                     'ratings':rating[:100]})\n",
    "    print(df)\n",
    "    df.to_csv('IMDB_top_100.csv', index = False)\n",
    "\n",
    "# Calling Function\n",
    "IMDB_top_100(\"https://www.imdb.com/chart/top/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3. Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. Name, IMDB rating, Year of release) and save it in form of a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          names   years  ratings\n",
      "0               Pather Panchali  (1955)      8.5\n",
      "1                      Gol Maal  (1979)      8.5\n",
      "2                      Ratsasan  (2018)      8.5\n",
      "3                       Nayakan  (1987)      8.5\n",
      "4                    Anbe Sivam  (2003)      8.5\n",
      "..                          ...     ...      ...\n",
      "95                  Section 375  (2019)      8.0\n",
      "96         Lage Raho Munna Bhai  (2006)      8.0\n",
      "97                      Deewaar  (1975)      8.0\n",
      "98                       Indian  (1996)      8.0\n",
      "99  Dilwale Dulhania Le Jayenge  (1995)      8.0\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function Definition\n",
    "def Indian_IMDB_top_100(url):\n",
    "    r=requests.get(url)\n",
    "    soup=BeautifulSoup(r.content,'html.parser')\n",
    "    name=[]\n",
    "    year=[]\n",
    "    rating=[]\n",
    "    for i in soup.find_all('img'):\n",
    "        name.append(i.attrs['alt'])\n",
    "    for i in list(soup.find_all('span',attrs={\"secondaryInfo\"})):\n",
    "        year.append(i.text)\n",
    "    for i in soup.find_all('strong'):\n",
    "        rating.append(float(i.text))\n",
    "    df=pd.DataFrame({'names':name[:100],\n",
    "                     'years':year[:100],\n",
    "                     'ratings':rating[:100]})\n",
    "    print(df)\n",
    "    df.to_csv('indian_IMDB_top_100.csv', index = False)\n",
    "\n",
    "# Calling Function\n",
    "Indian_IMDB_top_100(\"https://www.imdb.com/india/top-rated-indian-movies/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4. Write a python program to scrap book name, author name, genre and book review of any 5 books from ‘www.bookpage.com’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Name</th>\n",
       "      <th>Author</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paper Bullets</td>\n",
       "      <td>Jeffrey H. Jackson</td>\n",
       "      <td>Nonfiction / History / European History</td>\n",
       "      <td>Although it’s been 75 years since the end of W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>White Ivy</td>\n",
       "      <td>Susie Yang</td>\n",
       "      <td>Fiction / Coming of Age</td>\n",
       "      <td>Ivy Lin is no monster, but sometimes, when suf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Benson’s Beetle</td>\n",
       "      <td>Rachel Joyce</td>\n",
       "      <td>Fiction / Historical Fiction</td>\n",
       "      <td>Rachel Joyce’s first novel, The Unlikely Pilgr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>★ To Hold Up the Sky</td>\n",
       "      <td>Cixin Liu</td>\n",
       "      <td>Science Fiction &amp; Fantasy / Science Fiction / ...</td>\n",
       "      <td>Short stories in science fiction are frequentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>★ The Book Collectors</td>\n",
       "      <td>Delphine Minoui</td>\n",
       "      <td>Nonfiction / Biography / World Politics</td>\n",
       "      <td>“Books bring us closer together. They’re a bri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Book Name              Author  \\\n",
       "0          Paper Bullets  Jeffrey H. Jackson   \n",
       "1              White Ivy          Susie Yang   \n",
       "2   Miss Benson’s Beetle        Rachel Joyce   \n",
       "3   ★ To Hold Up the Sky           Cixin Liu   \n",
       "4  ★ The Book Collectors     Delphine Minoui   \n",
       "\n",
       "                                               Genre  \\\n",
       "0            Nonfiction / History / European History   \n",
       "1                            Fiction / Coming of Age   \n",
       "2                       Fiction / Historical Fiction   \n",
       "3  Science Fiction & Fantasy / Science Fiction / ...   \n",
       "4            Nonfiction / Biography / World Politics   \n",
       "\n",
       "                                              Review  \n",
       "0  Although it’s been 75 years since the end of W...  \n",
       "1  Ivy Lin is no monster, but sometimes, when suf...  \n",
       "2  Rachel Joyce’s first novel, The Unlikely Pilgr...  \n",
       "3  Short stories in science fiction are frequentl...  \n",
       "4  “Books bring us closer together. They’re a bri...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function Definition\n",
    "def bookpage():\n",
    "    response = requests.get('https://bookpage.com/reviews/')\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    url_tags = soup.find_all('div', attrs = {'class': 'row-fluid article-row'})\n",
    "    urls = [i.find('h4').find_all('a')[0]['href'] for i in url_tags[0:5]]\n",
    "    book_dict = {}\n",
    "    book_dict[\"Book Name\"] = []\n",
    "    book_dict[\"Author\"] = []\n",
    "    book_dict[\"Genre\"] = []\n",
    "    book_dict[\"Review\"] = []\n",
    "    for url in urls:\n",
    "        book = requests.get('https://www.bookpage.com'+url)\n",
    "        soup = BeautifulSoup(book.content, 'html.parser')\n",
    "        book_dict[\"Book Name\"].append(soup.find('h1').text.replace('\\n',''))\n",
    "        book_dict[\"Author\"].append(soup.find('h4').text.replace('\\n',''))\n",
    "        book_dict[\"Genre\"].append(soup.find('p', attrs = {'class':'genre-links'}).text.replace('\\n',''))\n",
    "        book_dict[\"Review\"].append(soup.find('div', attrs = {'class':'article-body'}).text.replace('\\n',''))\n",
    "    book_df = pd.DataFrame.from_dict(book_dict)\n",
    "    book_df.to_csv('Book Reviews.csv', index = False)\n",
    "    return book_df\n",
    "\n",
    "# Calling Function\n",
    "bookpage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5. Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You have to scrape:\n",
    "i) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.                                 \n",
    "ii) Top 10 ODI Batsmen in men along with the records of their team and rating.                                             \n",
    "iii) Top 10 ODI bowlers along with the records of their team and rating.\n",
    "\n",
    "### Solution:\n",
    "i) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Team Match Points\n",
      "0       England    44  5,405\n",
      "1         India    49  5,819\n",
      "2   New Zealand    32  3,716\n",
      "3     Australia    36  3,941\n",
      "4  South Africa    31  3,345\n",
      "5      Pakistan    35  3,590\n",
      "6    Bangladesh    34  2,989\n",
      "7     Sri Lanka    39  3,297\n",
      "8   West Indies    43  3,285\n",
      "9   Afghanistan    28  1,549\n"
     ]
    }
   ],
   "source": [
    "# Function Definition\n",
    "def Top_10_ODI_team(url):\n",
    "    r=requests.get(url)\n",
    "    soup=BeautifulSoup(r.content,'html.parser')\n",
    "    team=[]\n",
    "    match=[]\n",
    "    rating=[]\n",
    "    for i in soup.find_all('span',attrs={'u-hide-phablet'}):\n",
    "        team.append(i.text)\n",
    "    match.append(soup.find('td',attrs={'rankings-block__banner--matches'}).text)\n",
    "    rating.append(soup.find('td',attrs={'rankings-block__banner--points'}).text)\n",
    "    count=0\n",
    "    for i in list(soup.find_all('td',attrs={'table-body__cell u-center-text'}))[:18]:\n",
    "        if count%2==0:\n",
    "            match.append(i.text)\n",
    "        else:\n",
    "            rating.append(i.text)\n",
    "        count+=1\n",
    "    df=pd.DataFrame({'Team':team[:10],\n",
    "                     'Match':match,\n",
    "                     'Points':rating})\n",
    "    df.to_csv('top_10_odi_teams_men.csv', index = False)\n",
    "    print(df)\n",
    "\n",
    "# Calling Function\n",
    "Top_10_ODI_team('https://www.icc-cricket.com/rankings/mens/team-rankings/odi')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Top 10 ODI Batsmen in men along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Name     Team points\n",
      "0          Virat Kohli  \\n\\nIND    871\n",
      "1         Rohit Sharma      IND    855\n",
      "2           Babar Azam      PAK    837\n",
      "3          Ross Taylor       NZ    818\n",
      "4  Francois du Plessis       SA    790\n",
      "5      Kane Williamson       NZ    765\n",
      "6          Aaron Finch      AUS    762\n",
      "7         David Warner      AUS    759\n",
      "8      Quinton de Kock       SA    755\n",
      "9       Jonny Bairstow      ENG    754\n"
     ]
    }
   ],
   "source": [
    "# Function Definition\n",
    "def Top_10_ODI_batsman(url):\n",
    "    r=requests.get(url)\n",
    "    soup=BeautifulSoup(r.content,'html.parser')\n",
    "    name=[]\n",
    "    team=[]\n",
    "    point=[]\n",
    "    name.append(soup.find('div',attrs={\"rankings-block__banner--name-large\"}).text)\n",
    "    for i in list(soup.find_all('td',attrs={'table-body__cell rankings-table__name name'}))[:9]:\n",
    "        name.append(i.text.replace('\\n',''))\n",
    "    team.append(soup.find('div',attrs={\"rankings-block__banner--nationality\"}).text)\n",
    "    for i in list(soup.find_all('td',attrs={'table-body__cell nationality-logo rankings-table__team'}))[:9]:\n",
    "        team.append(i.text.replace('\\n',''))\n",
    "    point.append(soup.find('div',attrs={\"rankings-block__banner--rating\"}).text)\n",
    "    for i in list(soup.find_all('td',attrs={'table-body__cell rating'}))[:9]:\n",
    "        point.append(i.text)\n",
    "    df=pd.DataFrame({'Name': name,\n",
    "                     'Team':team,\n",
    "                     'points':point})\n",
    "    df.to_csv('top_10_odi_batsman.csv', index = False)\n",
    "    print(df)\n",
    "\n",
    "# Calling Function\n",
    "Top_10_ODI_batsman(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Name    Team points\n",
      "0       Trent Boult  \\n\\nNZ    722\n",
      "1    Jasprit Bumrah     IND    719\n",
      "2  Mujeeb Ur Rahman     AFG    701\n",
      "3      Chris Woakes     ENG    675\n",
      "4     Kagiso Rabada      SA    665\n",
      "5       Pat Cummins     AUS    659\n",
      "6    Josh Hazlewood     AUS    654\n",
      "7     Mohammad Amir     PAK    647\n",
      "8        Matt Henry      NZ    641\n",
      "9      Jofra Archer     ENG    637\n"
     ]
    }
   ],
   "source": [
    "def Top_10_ODI_bowler(url):\n",
    "    r=requests.get(url)\n",
    "    soup=BeautifulSoup(r.content,'html.parser')\n",
    "    name=[]\n",
    "    team=[]\n",
    "    point=[]\n",
    "    name.append(soup.find('div',attrs={\"rankings-block__banner--name-large\"}).text)\n",
    "    for i in list(soup.find_all('td',attrs={'table-body__cell rankings-table__name name'}))[:9]:\n",
    "        name.append(i.text.replace('\\n',''))\n",
    "    team.append(soup.find('div',attrs={\"rankings-block__banner--nationality\"}).text)\n",
    "    for i in list(soup.find_all('td',attrs={'table-body__cell nationality-logo rankings-table__team'}))[:9]:\n",
    "        team.append(i.text.replace('\\n',''))\n",
    "    point.append(soup.find('div',attrs={\"rankings-block__banner--rating\"}).text)\n",
    "    for i in list(soup.find_all('td',attrs={'table-body__cell rating'}))[:9]:\n",
    "        point.append(i.text)\n",
    "    df=pd.DataFrame({'Name': name,\n",
    "                     'Team':team,\n",
    "                     'points':point})\n",
    "    df.to_csv('top_10_odi_bowler.csv', index = False)\n",
    "    print(df)\n",
    "\n",
    "# Calling Function\n",
    "Top_10_ODI_batsman(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6. Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You have to scrape:\n",
    "i) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.                                  \n",
    "ii) Top 10 women’s ODI players along with the records of their team and rating.                                                \n",
    "iii) Top 10 women’s ODI all-rounder along with the records of their team and rating. \n",
    "\n",
    "#### Solutions:\n",
    "i) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Team Match Points\n",
      "0     Australia    15  2,436\n",
      "1         India    15  1,812\n",
      "2       England    14  1,670\n",
      "3  South Africa    16  1,713\n",
      "4   New Zealand    15  1,384\n",
      "5   West Indies    12  1,025\n",
      "6      Pakistan    12    927\n",
      "7    Bangladesh     5    306\n",
      "8     Sri Lanka    11    519\n",
      "9       Ireland     2     25\n"
     ]
    }
   ],
   "source": [
    "# Function Definition\n",
    "def Top_10_Women_ODI_team(url):\n",
    "    r=requests.get(url)\n",
    "    soup=BeautifulSoup(r.content,'html.parser')\n",
    "    team=[]\n",
    "    match=[]\n",
    "    rating=[]\n",
    "    for i in soup.find_all('span',attrs={'u-hide-phablet'}):\n",
    "        team.append(i.text)\n",
    "    match.append(soup.find('td',attrs={'rankings-block__banner--matches'}).text)\n",
    "    rating.append(soup.find('td',attrs={'rankings-block__banner--points'}).text)\n",
    "    count=0\n",
    "    for i in list(soup.find_all('td',attrs={'table-body__cell u-center-text'}))[:18]:\n",
    "        if count%2==0:\n",
    "            match.append(i.text)\n",
    "        else:\n",
    "            rating.append(i.text)\n",
    "        count+=1\n",
    "    df=pd.DataFrame({'Team':team[:10],\n",
    "                     'Match':match,\n",
    "                     'Points':rating})\n",
    "    df.to_csv('top_10_women_odi_teams.csv', index = False)\n",
    "    print(df)\n",
    "\n",
    "# Calling Function\n",
    "Top_10_Women_ODI_team('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Top 10 women’s ODI players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Name     Team points\n",
      "0        Meg Lanning  \\n\\nAUS    749\n",
      "1    Stafanie Taylor       WI    746\n",
      "2       Alyssa Healy      AUS    741\n",
      "3    Smriti Mandhana      IND    732\n",
      "4  Amy Satterthwaite       NZ    723\n",
      "5     Tammy Beaumont      ENG    716\n",
      "6       Ellyse Perry      AUS    691\n",
      "7        Lizelle Lee       SA    690\n",
      "8    Laura Wolvaardt       SA    689\n",
      "9        Mithali Raj      IND    687\n"
     ]
    }
   ],
   "source": [
    "# Function Definition\n",
    "def Top_10_Women_ODI_player(url):\n",
    "    r=requests.get(url)\n",
    "    soup=BeautifulSoup(r.content,'html.parser')\n",
    "    name=[]\n",
    "    team=[]\n",
    "    point=[]\n",
    "    name.append(soup.find('div',attrs={\"rankings-block__banner--name-large\"}).text)\n",
    "    for i in list(soup.find_all('td',attrs={'table-body__cell rankings-table__name name'}))[:9]:\n",
    "        name.append(i.text.replace('\\n',''))\n",
    "    team.append(soup.find('div',attrs={\"rankings-block__banner--nationality\"}).text)\n",
    "    for i in list(soup.find_all('td',attrs={'table-body__cell nationality-logo rankings-table__team'}))[:9]:\n",
    "        team.append(i.text.replace('\\n',''))\n",
    "    point.append(soup.find('div',attrs={\"rankings-block__banner--rating\"}).text)\n",
    "    for i in list(soup.find_all('td',attrs={'table-body__cell rating'}))[:9]:\n",
    "        point.append(i.text)\n",
    "    df=pd.DataFrame({'Name': name,\n",
    "                     'Team':team,\n",
    "                     'points':point})\n",
    "    df.to_csv('top_10_women_odi_player.csv', index = False)\n",
    "    print(df)\n",
    "\n",
    "# Calling Function\n",
    "Top_10_Women_ODI_player(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Name     Team points\n",
      "0      Ellyse Perry  \\n\\nAUS    460\n",
      "1   Stafanie Taylor       WI    410\n",
      "2    Marizanne Kapp       SA    389\n",
      "3     Deepti Sharma      IND    359\n",
      "4  Dane van Niekerk       SA    335\n",
      "5     Jess Jonassen      AUS    301\n",
      "6     Sophie Devine       NZ    289\n",
      "7    Natalie Sciver      ENG    273\n",
      "8     Shikha Pandey      IND    250\n",
      "9   Katherine Brunt      ENG    232\n"
     ]
    }
   ],
   "source": [
    "#Function Definition\n",
    "def Top_10_Women_ODI_Allrounder(url):\n",
    "    r=requests.get(url)\n",
    "    soup=BeautifulSoup(r.content,'html.parser')\n",
    "    name=[]\n",
    "    team=[]\n",
    "    point=[]\n",
    "    name.append(soup.find('div',attrs={\"rankings-block__banner--name-large\"}).text)\n",
    "    for i in list(soup.find_all('td',attrs={'table-body__cell rankings-table__name name'}))[:9]:\n",
    "        name.append(i.text.replace('\\n',''))\n",
    "    team.append(soup.find('div',attrs={\"rankings-block__banner--nationality\"}).text)\n",
    "    for i in list(soup.find_all('td',attrs={'table-body__cell nationality-logo rankings-table__team'}))[:9]:\n",
    "        team.append(i.text.replace('\\n',''))\n",
    "    point.append(soup.find('div',attrs={\"rankings-block__banner--rating\"}).text)\n",
    "    for i in list(soup.find_all('td',attrs={'table-body__cell rating'}))[:9]:\n",
    "        point.append(i.text)\n",
    "    df=pd.DataFrame({'Name': name,\n",
    "                     'Team':team,\n",
    "                     'points':point})\n",
    "    df.to_csv('top_10_women_odi_allrounder.csv', index = False)\n",
    "    print(df)\n",
    "\n",
    "# Calling function\n",
    "Top_10_Women_ODI_Allrounder(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7. Write a python program to scrape details of all the mobile phones under Rs. 20,000 listed on Amazon.in. The scraped data should include Product Name, Price, Image URL and Average Rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Name      Price  \\\n",
      "0    Redmi Note 9 Pro (Champagne Gold, 4GB RAM, 128...  13,999.00   \n",
      "1    Samsung Galaxy M01 (Black, 3GB RAM, 32GB Stora...   7,999.00   \n",
      "2    Samsung Galaxy M21 (Midnight Blue, 4GB RAM, 64...  12,499.00   \n",
      "3    Redmi 8A Dual (Sea Blue, 3GB RAM, 32GB Storage...   7,999.00   \n",
      "4    Samsung Galaxy M31s (Mirage Blue, 6GB RAM, 128...  18,499.00   \n",
      "..                                                 ...        ...   \n",
      "125  Vivo Y91i (Fusion Black, 2GB RAM, 32GB Storage...   7,990.00   \n",
      "126  Vivo Y91i (Ocean Blue, 2GB RAM, 32GB Storage) ...   7,990.00   \n",
      "127  Samsung Galaxy M31 (Ocean Blue, 6GB RAM, 64GB ...  15,499.00   \n",
      "128  Tecno Spark 6 Air (Ocean Blue, 3GB RAM, 32GB S...   9,999.00   \n",
      "129                 Nokia 105 2019 (Single SIM, Black)   1,249.00   \n",
      "\n",
      "                 Rating                                         Image_link  \n",
      "0    4.2 out of 5 stars  https://images-na.ssl-images-amazon.com/images...  \n",
      "1    3.9 out of 5 stars  https://images-na.ssl-images-amazon.com/images...  \n",
      "2    4.2 out of 5 stars  https://images-na.ssl-images-amazon.com/images...  \n",
      "3    4.0 out of 5 stars  https://images-na.ssl-images-amazon.com/images...  \n",
      "4    4.3 out of 5 stars  https://images-na.ssl-images-amazon.com/images...  \n",
      "..                  ...                                                ...  \n",
      "125  4.2 out of 5 stars  https://images-na.ssl-images-amazon.com/images...  \n",
      "126  4.1 out of 5 stars  https://images-na.ssl-images-amazon.com/images...  \n",
      "127  4.2 out of 5 stars  https://images-na.ssl-images-amazon.com/images...  \n",
      "128  3.9 out of 5 stars  https://images-na.ssl-images-amazon.com/images...  \n",
      "129  4.1 out of 5 stars  https://images-na.ssl-images-amazon.com/images...  \n",
      "\n",
      "[130 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function Definition\n",
    "def amazon_mob(url):\n",
    "    driver=webdriver.Chrome('chromedriver.exe')\n",
    "    start_page=0\n",
    "    end_page=4\n",
    "    urls = []\n",
    "    name=[]\n",
    "    price=[]\n",
    "    image=[]\n",
    "    rating=[]\n",
    "    #loop to fetch urls of each mobile till page 5\n",
    "    for page in range(start_page,end_page+1):\n",
    "        driver.get(url)\n",
    "        soup= BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        prod_urls = soup.find_all('a', attrs ={'class':'a-link-normal a-text-normal'})\n",
    "        for prod in prod_urls:\n",
    "            urls.append('https://www.amazon.in'+prod.get('href'))\n",
    "    \n",
    "    #loop to scrap required details from each mobile page\n",
    "    for url in urls:\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        n = soup.find('h1',attrs={'id':'title'})\n",
    "        if n is not None:\n",
    "            name.append(n.find('span').text.replace('\\n',''))\n",
    "        else:\n",
    "            name.append('-')\n",
    "        rat = soup.find('div', attrs = {'id':'averageCustomerReviews'})\n",
    "        if rat is not None:\n",
    "            rating.append(rat.find('i').find('span').text)\n",
    "        else:\n",
    "            rating.append('-')\n",
    "        p = soup.find('span', attrs = {'class':'a-size-medium a-color-price priceBlockDealPriceString'})\n",
    "        if p is not None:\n",
    "            price.append(p.text[2:])\n",
    "        else:\n",
    "            p = soup.find('div', attrs = {'id':'price'})\n",
    "            if p is not None:\n",
    "                price.append(p.find('span').text[2:])\n",
    "            else:\n",
    "                price.append('-')\n",
    "        img = soup.find('div', attrs = {'class':'imgTagWrapper'})\n",
    "        if img is not None:\n",
    "            image.append(img.find('img').get('src'))\n",
    "        else:\n",
    "            image.append('-')\n",
    "    mob_df = df=pd.DataFrame({'Name':name,\n",
    "                              'Price':price,\n",
    "                              'Rating':rating,\n",
    "                              'Image_link':image})\n",
    "    print(mob_df)\n",
    "    mob_df.to_csv('Amazon Mobiles.csv', index = False)\n",
    "    \n",
    "    \n",
    "# Calling Function\n",
    "amazon_mob('https://www.amazon.in/s?k=mobile+phones+under+20000&rh=n%3A1389432031&dc&qid=1604132579&rnid=3576079031&ref=sr_nr_n_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8. Write a python program to extract information about the local weather from the National Weather Service website of USA, https://www.weather.gov/ for the city, San Francisco. You need to extract data about 7 day extended forecast display for the city. The data should include period, short description, temperature and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                period   short_description  temperature  \\\n",
      "0  NOW until6:00am Sat  High Surf Advisory                \n",
      "1            Overnight       ChanceShowers   Low: 49 °F   \n",
      "2             Saturday               Sunny  High: 60 °F   \n",
      "3        SaturdayNight        Mostly Clear   Low: 50 °F   \n",
      "4               Sunday               Sunny  High: 60 °F   \n",
      "5          SundayNight        Mostly Clear   Low: 46 °F   \n",
      "6               Monday        Mostly Sunny  High: 58 °F   \n",
      "7          MondayNight       Partly Cloudy   Low: 45 °F   \n",
      "8              Tuesday        Mostly Sunny  High: 62 °F   \n",
      "\n",
      "                                         description  \n",
      "0  A 30 percent chance of showers.  Partly cloudy...  \n",
      "1  Sunny, with a high near 60. Light and variable...  \n",
      "2  Mostly clear, with a low around 50. Northwest ...  \n",
      "3  Sunny, with a high near 60. North northwest wi...  \n",
      "4  Mostly clear, with a low around 46. North nort...  \n",
      "5                 Mostly sunny, with a high near 58.  \n",
      "6               Partly cloudy, with a low around 45.  \n",
      "7                 Mostly sunny, with a high near 62.  \n",
      "8               Partly cloudy, with a low around 47.  \n"
     ]
    }
   ],
   "source": [
    "# Function Definition\n",
    "def weather(url):\n",
    "    r=requests.get(url)\n",
    "    soup=BeautifulSoup(r.content,'html.parser')\n",
    "    period=[]\n",
    "    short_desc=[]\n",
    "    temp=[]\n",
    "    desc=[]\n",
    "    for i in soup.find_all('p',attrs={'period-name'}):\n",
    "        period.append(i.text)\n",
    "    for i in soup.find_all('p',attrs={'short-desc'}):\n",
    "        short_desc.append(i.text)\n",
    "    for i in soup.find_all('p',attrs={'short-desc'}):\n",
    "        if i.next_sibling is not None:\n",
    "            temp.append(i.next_sibling.text)\n",
    "        else:\n",
    "            temp.append(' ')\n",
    "    for i in soup.find_all('div',attrs={\"col-sm-10 forecast-text\"}):\n",
    "        desc.append(i.text)\n",
    "    df=pd.DataFrame({\"period\":period,\n",
    "                     \"short_description\":short_desc,\n",
    "                     \"temperature\":temp,\n",
    "                     \"description\":desc[:9]})\n",
    "    df.to_csv('Extended_forecast.csv')\n",
    "    print(df)\n",
    "\n",
    "# Calling Function\n",
    "weather(\"https://forecast.weather.gov/MapClick.php?lat=37.777120000000025&lon=-122.41963999999996#.X6Pyxe3hWMo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9. Write a python program to scrape ‘software developer’ job listings from ‘Monster.com’. It should include all the jobs listed for the next 5 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Job_Names  \\\n",
      "0                 Hiring \"Software Developer\" - Mumbai   \n",
      "1                                      Api Development   \n",
      "2                                    Android Developer   \n",
      "3                          Gunank Technologies PVT LTD   \n",
      "4                                   Software Developer   \n",
      "..                                                 ...   \n",
      "120                                 Software Developer   \n",
      "121                         Frontend Software Engineer   \n",
      "122  URGENT REQUIRED 15 JAVA DEVELOPERS (FRESHER) F...   \n",
      "123  Senior Development Engineer In Test (SDET) - A...   \n",
      "124  URGENT REQUIRED 15 JAVA DEVELOPERS (FRESHER) F...   \n",
      "\n",
      "                                             Company  \\\n",
      "0                                        HR Dynamics   \n",
      "1          TalentFirst HR Consulting Private Limited   \n",
      "2                     Smit Job Placement Consultancy   \n",
      "3                Gunank Technologies Private Limited   \n",
      "4                               DXC Technology India   \n",
      "..                                               ...   \n",
      "120                 Orange Technolab Private Limited   \n",
      "121                             India Manpower Group   \n",
      "122  TRANSVISION SOFTWARE AND DATA SOLUTIONS PVT LTD   \n",
      "123             Vitestork Consulting Private Limited   \n",
      "124  TRANSVISION SOFTWARE AND DATA SOLUTIONS PVT LTD   \n",
      "\n",
      "                                              location  \\\n",
      "0                                    Mumbai, Mumbai...   \n",
      "1                                    Bengaluru / Ba...   \n",
      "2                                                Delhi   \n",
      "3                                            Singrauli   \n",
      "4                                    Bengaluru / Ba...   \n",
      "..                                                 ...   \n",
      "120                                          Ahmedabad   \n",
      "121                                          Mangalore   \n",
      "122                                  Bengaluru / Ba...   \n",
      "123                                  Bengaluru / Ba...   \n",
      "124                                  Bengaluru / Ba...   \n",
      "\n",
      "                                     Experience  \\\n",
      "0                                   2-6 Years     \n",
      "1                                  8-12 Years     \n",
      "2                                   0-2 Years     \n",
      "3                                   3-4 Years     \n",
      "4                                   5-9 Years     \n",
      "..                                          ...   \n",
      "120                                 2-6 Years     \n",
      "121                                 0-3 Years     \n",
      "122                                 0-1 Years     \n",
      "123                                8-15 Years     \n",
      "124                                 0-1 Years     \n",
      "\n",
      "                                                Salary  \n",
      "0                                        Not Specified  \n",
      "1                                        Not Specified  \n",
      "2                                    1,70,000-3,00,...  \n",
      "3                                    20,000-30,000 INR  \n",
      "4                                        Not Specified  \n",
      "..                                                 ...  \n",
      "120                                  3,00,000-7,00,...  \n",
      "121                                  3,00,000-6,30,...  \n",
      "122                                      Not Specified  \n",
      "123                                      Not Specified  \n",
      "124                                      Not Specified  \n",
      "\n",
      "[125 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function Definition\n",
    "def soft_dev(url):\n",
    "    driver=webdriver.Chrome('chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    start_page=0\n",
    "    end_page=4\n",
    "    urls=[]\n",
    "    job_names=[]\n",
    "    company=[]\n",
    "    location=[]\n",
    "    experience=[]\n",
    "    salary=[]\n",
    "    for page in range(start_page,end_page+1):\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        jobs=soup.find_all('div', attrs ={'class':'job-tittle'})\n",
    "        for job in jobs:\n",
    "            l=job.text.split('\\n')\n",
    "            job_names.append(l[0].split('  ')[0])\n",
    "            company.append(l[0].split('  ')[1])\n",
    "            location.append(l[1])\n",
    "            experience.append(l[2])\n",
    "            salary.append(l[3])\n",
    "    \n",
    "        nxt_button=driver.find_element_by_xpath(\"//button[@class='btn-next-prev btn-next']\")\n",
    "        if nxt_button.text=='Next':\n",
    "            nxt_button.click()\n",
    "            time.sleep(5)\n",
    "\n",
    "    job_df=pd.DataFrame({'Job_Names':job_names,\n",
    "                         'Company':company,\n",
    "                         'location':location,\n",
    "                         'Experience':experience,\n",
    "                         'Salary':salary})\n",
    "    job_df.to_csv('Software Developer_Monster', index = False)\n",
    "    print(job_df)\n",
    "\n",
    "# Calling Function\n",
    "soft_dev(\"https://www.monsterindia.com/srp/results?query=software%20developer&searchId=1eef9a54-8213-4bcf-9ef7-8605ec8202c5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10. Write a python program to scrape ‘data scientist’ job listings for location ‘New Delhi’ from ‘Monster.com’. It should include all the jobs listed for the next 5 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Job_Names  \\\n",
      "0                                   Data Engineer - AI   \n",
      "1                                        Data Engineer   \n",
      "2                                          Urgent Mass   \n",
      "3                             Data Analyst / Fresher /   \n",
      "4                             Data Analyst / Fresher /   \n",
      "..                                                 ...   \n",
      "120                      Data migration tool developer   \n",
      "121  Urgent mass hiring for data entry operator Bac...   \n",
      "122  SNAPDEAL DATA ENTRY OPERATOR PROCESS Fresher c...   \n",
      "123  urgent mass hiring for Data entry operations ,...   \n",
      "124                                US Pharma recruiter   \n",
      "\n",
      "                                Company  \\\n",
      "0                      Snaphunt Pte Ltd   \n",
      "1                      Snaphunt Pte Ltd   \n",
      "2                                Hiring   \n",
      "3     Data Scientist / Graduate Fresher   \n",
      "4     Data Scientist / Graduate Fresher   \n",
      "..                                  ...   \n",
      "120         URS Systems Private Limited   \n",
      "121                         Call 2 Hire   \n",
      "122                     NKV Enterprises   \n",
      "123                         Call 2 Hire   \n",
      "124   US Tech Solutions Private Limited   \n",
      "\n",
      "                                              location  \\\n",
      "0                                    Bengaluru / Ba...   \n",
      "1                                    Bengaluru / Ba...   \n",
      "2                                         Delhi, Noida   \n",
      "3                                     Delhi, Ghaziabad   \n",
      "4                                     Delhi, Ghaziabad   \n",
      "..                                                 ...   \n",
      "120                                  Bengaluru / Ba...   \n",
      "121                                  Delhi, Gurgaon...   \n",
      "122                                  Delhi, Gurgaon...   \n",
      "123                                  Delhi, Gurgaon...   \n",
      "124                                       Delhi, Noida   \n",
      "\n",
      "                                     Experience  \\\n",
      "0                                   0-5 Years     \n",
      "1                                   2-7 Years     \n",
      "2                                   0-2 Years     \n",
      "3                                   0-5 Years     \n",
      "4                                   0-5 Years     \n",
      "..                                          ...   \n",
      "120                                7-12 Years     \n",
      "121                                   Fresher     \n",
      "122                                 0-5 Years     \n",
      "123                                   Fresher     \n",
      "124                                 1-3 Years     \n",
      "\n",
      "                                                Salary  \n",
      "0                                        Not Specified  \n",
      "1                                        Not Specified  \n",
      "2                                        Not Specified  \n",
      "3                                        Not Specified  \n",
      "4                                        Not Specified  \n",
      "..                                                 ...  \n",
      "120                                      Not Specified  \n",
      "121                                      Not Specified  \n",
      "122                                  1,80,000-2,70,...  \n",
      "123                                      Not Specified  \n",
      "124                                      Not Specified  \n",
      "\n",
      "[125 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function Definition\n",
    "def data_sci(url):\n",
    "    driver=webdriver.Chrome('chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    start_page=0\n",
    "    end_page=4\n",
    "    urls=[]\n",
    "    job_names=[]\n",
    "    company=[]\n",
    "    location=[]\n",
    "    experience=[]\n",
    "    salary=[]\n",
    "    for page in range(start_page,end_page+1):\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        jobs=soup.find_all('div', attrs ={'class':'job-tittle'})\n",
    "        for job in jobs:\n",
    "            l=job.text.split('\\n')\n",
    "            job_names.append(l[0].split('  ')[0])\n",
    "            company.append(l[0].split('  ')[1])\n",
    "            location.append(l[1])\n",
    "            experience.append(l[2])\n",
    "            salary.append(l[3])\n",
    "    \n",
    "        nxt_button=driver.find_element_by_xpath(\"//button[@class='btn-next-prev btn-next']\")\n",
    "        if nxt_button.text=='Next':\n",
    "            nxt_button.click()\n",
    "            time.sleep(5)\n",
    "\n",
    "    job_df=pd.DataFrame({'Job_Names':job_names,\n",
    "                         'Company':company,\n",
    "                         'location':location,\n",
    "                         'Experience':experience,\n",
    "                         'Salary':salary})\n",
    "    job_df.to_csv('Data Scientist_ND_Monster', index = False)\n",
    "    print(job_df)\n",
    "\n",
    "# Calling Function\n",
    "data_sci(\"https://www.monsterindia.com/srp/results?query=Data%20Scientist&locations=Delhi&searchId=ebc9a3bb-97c5-4647-8eac-ad5d9d92d3fc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Document"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
