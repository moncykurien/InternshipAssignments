{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selenium libraries\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from configparser import ConfigParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a common launch pad for chrome browser with Webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_drive_launch(url):\n",
    "    \"\"\"\n",
    "    This function initiates the Chrome browser's driver with the url passed as a parameter and returns the driver instance.\n",
    "    Parameters:\n",
    "        url - url of the website\n",
    "    return:\n",
    "        driver - driver instance of the chrome browser    \n",
    "    \"\"\"\n",
    "    #creating driver instance\n",
    "    driver = webdriver.Chrome('./driver/chromedriver.exe')\n",
    "    driver.maximize_window()\n",
    "    \n",
    "    #defining implicit wait\n",
    "    driver.implicitly_wait(10)\n",
    "    \n",
    "    #launching the url\n",
    "    driver.get(url)\n",
    "    \n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data. \n",
    "\n",
    "Note- All of the above steps have to be done in code. No step is to be done manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highlevel_details_naukri(job, location = None, salary_range=None, stop=None, use_filter=False):\n",
    "    \"\"\"\n",
    "    This function will fetch the highlevel details from naukri website for the job tile and location provided as parameter values\n",
    "        The function takes the \n",
    "        job - job_title\n",
    "        location - default=None. The location filter\n",
    "        salary_range - default = None. the salary filter\n",
    "        stop - number of jobs to fetch. default =None\n",
    "        use_filter - If filter facet should be used. default None\n",
    "    \"\"\"\n",
    "    titles=[]\n",
    "    companies=[]\n",
    "    exps=[]\n",
    "    salaries=[]\n",
    "    locs=[]\n",
    "\n",
    "    #launching naukri site\n",
    "    url = 'https://www.naukri.com/'\n",
    "    drivr = get_drive_launch(url)\n",
    "    \n",
    "    #typing in the job title to search\n",
    "    drivr.find_element_by_id(\"qsb-keyword-sugg\").send_keys(job)\n",
    "    \n",
    "    #checking if location filter should be used directly or in the filter facet\n",
    "    if ((location) and (not use_filter)):\n",
    "        drivr.find_element_by_id('qsb-location-sugg').send_keys(location)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    #clicking on search\n",
    "    drivr.find_element_by_xpath(\"//div[@class='search-btn']/button\").click()\n",
    "    \n",
    "    #checking if filter for the location\n",
    "    if use_filter:        \n",
    "        if location:\n",
    "            time.sleep(3)\n",
    "            try:\n",
    "                drivr.find_element_by_xpath(\"//span[@class = 'ellipsis fleft' and contains(text(), \"+\"'\"+location+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "            except:\n",
    "                drivr.find_element_by_xpath(\"//div[@data-filter-id='citiesGid']//a\").click()\n",
    "                time.sleep(3)\n",
    "                drivr.find_element_by_xpath(\"//span[@class = 'ellipsis fleft' and contains(text(), \"+\"'\"+location+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "    \n",
    "    #checking if salary range is requested\n",
    "    if salary_range:\n",
    "            try:\n",
    "                drivr.find_element_by_xpath(\"//span[contains(text(), \"+\"'\"+salary_range+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "            except:\n",
    "                drivr.find_element_by_xpath(\"//div[@data-filter-id='salaryRange']//a\").click()\n",
    "                time.sleep(3)\n",
    "                drivr.find_element_by_xpath(\"//span[contains(text(), \"+\"'\"+salary_range+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    #printing out the topic for search results\n",
    "    print(drivr.find_element_by_xpath(\"//div[@class='sortAndH1Cont']/h1\").text)\n",
    "\n",
    "    #Fetching the web elements\n",
    "    job_titles = drivr.find_elements_by_xpath(\"//div[@class='info fleft']/a\")\n",
    "    job_companies = drivr.find_elements_by_xpath(\"//div[@class='info fleft']/div/a[1]\")\n",
    "    job_exps = drivr.find_elements_by_xpath(\"//div[@class='info fleft']/ul/li[1]\")\n",
    "    job_salaries = drivr.find_elements_by_xpath(\"//div[@class='info fleft']/ul/li[2]\")\n",
    "    job_locations = drivr.find_elements_by_xpath(\"//div[@class='info fleft']/ul/li[3]\")\n",
    "    \n",
    "    #iterating through the webelements to scrap the texts\n",
    "    for job_title, job_company, job_exp, job_salary, job_loc in zip(job_titles[:stop],job_companies[:stop],job_exps[:stop],job_salaries[:stop],job_locations[:stop]):\n",
    "        titles.append(job_title.text)\n",
    "        companies.append(job_company.text)\n",
    "        exps.append(job_exp.text)\n",
    "        salaries.append(job_salary.text)\n",
    "        locs.append(job_loc.text)\n",
    "    \n",
    "    #returning the driver instance and the data as a dataframe\n",
    "    return drivr, pd.DataFrame({'Job Title':titles, 'Company':companies, 'Experience Required':exps,'Salary offered':salaries, 'Location':locs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Analyst Jobs In Bangalore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience Required</th>\n",
       "      <th>Salary offered</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst Analyzing</td>\n",
       "      <td>Cistup Indian Institute of Science</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Analyst-Data Visualization</td>\n",
       "      <td>Accenture Solutions Pvt Ltd</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Market Unit - Data Business Analyst (11)</td>\n",
       "      <td>Accenture Solutions Pvt Ltd</td>\n",
       "      <td>1-2 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Myntra Designs Pvt. Ltd.</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Analyst-Finance Data Maintenance</td>\n",
       "      <td>Accenture Solutions Pvt Ltd</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Analyst-Data Management</td>\n",
       "      <td>Accenture Solutions Pvt Ltd</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Analyst-Data Management</td>\n",
       "      <td>Accenture Solutions Pvt Ltd</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Analyst-Data Management</td>\n",
       "      <td>Accenture Solutions Pvt Ltd</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Analyst-Data Visualization</td>\n",
       "      <td>Accenture Solutions Pvt Ltd</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Analyst-Finance Data Maintenance</td>\n",
       "      <td>Accenture Solutions Pvt Ltd</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Job Title  \\\n",
       "0                    Data Analyst Analyzing   \n",
       "1         Senior Analyst-Data Visualization   \n",
       "2  Market Unit - Data Business Analyst (11)   \n",
       "3                              Data Analyst   \n",
       "4          Analyst-Finance Data Maintenance   \n",
       "5            Senior Analyst-Data Management   \n",
       "6                   Analyst-Data Management   \n",
       "7            Senior Analyst-Data Management   \n",
       "8         Senior Analyst-Data Visualization   \n",
       "9   Senior Analyst-Finance Data Maintenance   \n",
       "\n",
       "                              Company Experience Required Salary offered  \\\n",
       "0  Cistup Indian Institute of Science             2-5 Yrs  Not disclosed   \n",
       "1         Accenture Solutions Pvt Ltd             5-8 Yrs  Not disclosed   \n",
       "2         Accenture Solutions Pvt Ltd             1-2 Yrs  Not disclosed   \n",
       "3            Myntra Designs Pvt. Ltd.             3-6 Yrs  Not disclosed   \n",
       "4         Accenture Solutions Pvt Ltd             3-5 Yrs  Not disclosed   \n",
       "5         Accenture Solutions Pvt Ltd             5-8 Yrs  Not disclosed   \n",
       "6         Accenture Solutions Pvt Ltd             3-5 Yrs  Not disclosed   \n",
       "7         Accenture Solutions Pvt Ltd             5-8 Yrs  Not disclosed   \n",
       "8         Accenture Solutions Pvt Ltd             5-8 Yrs  Not disclosed   \n",
       "9         Accenture Solutions Pvt Ltd             5-8 Yrs  Not disclosed   \n",
       "\n",
       "              Location  \n",
       "0  Bangalore/Bengaluru  \n",
       "1  Bangalore/Bengaluru  \n",
       "2  Bangalore/Bengaluru  \n",
       "3  Bangalore/Bengaluru  \n",
       "4  Bangalore/Bengaluru  \n",
       "5  Bangalore/Bengaluru  \n",
       "6  Bangalore/Bengaluru  \n",
       "7  Bangalore/Bengaluru  \n",
       "8  Bangalore/Bengaluru  \n",
       "9  Bangalore/Bengaluru  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, dataFrame = get_highlevel_details_naukri('Data Analyst', location='Bangalore', stop=10)\n",
    "#quiting the driver instance\n",
    "d.quit()\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note- \n",
    "1. All of the above steps have to be done in code. No step is to be done manually.\n",
    "2. Please note that you have to scrape full job description. For that you may have to open each job separately as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_descrp_naukri(driver, stop = None):\n",
    "    \"\"\"\n",
    "    This function will go into each job ads and fetch the full job descriptions form naukri site.\n",
    "    It takes the \n",
    "    'driver' - Webdriver for chrome instance as a parameter\n",
    "    'stop' - stop - number of jobs to fetch. default =None\n",
    "    Returns the driver instance and a list of full descriptions\n",
    "    \n",
    "    \"\"\"\n",
    "    full_descriptions = []\n",
    "    \n",
    "    #get the window handles\n",
    "    main_window = driver.current_window_handle\n",
    "    all_windows = driver.window_handles\n",
    "    \n",
    "    #close the other windows except the main window. \n",
    "    for window in all_windows:\n",
    "        if window != main_window:\n",
    "            driver.switch_to.window(window)\n",
    "            driver.close()\n",
    "\n",
    "    driver.switch_to.window(main_window)\n",
    "    main_window = driver.current_window_handle\n",
    "    \n",
    "    #get the webelements and iterate through each webelement to scrap the data.\n",
    "    elems = driver.find_elements_by_xpath(\"//div[@class='info fleft']/a\")\n",
    "    for element in elems[:stop]:\n",
    "        element.click()\n",
    "        time.sleep(2)\n",
    "        \n",
    "        #get the window handles and switch to the new tab\n",
    "        all_windows = driver.window_handles\n",
    "        driver.switch_to.window(all_windows[1])\n",
    "        #scrap data\n",
    "        try:\n",
    "            full_descriptions.append(driver.find_element_by_xpath(\"//section[@class='job-desc']\").text)\n",
    "        except:\n",
    "            full_descriptions.append(driver.find_element_by_xpath(\"//div[@class='clearboth description']\").text)\n",
    "        #close the new tab and go back to main window\n",
    "        driver.close()\n",
    "        driver.switch_to.window(main_window)\n",
    "    \n",
    "    return driver, full_descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Full Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>CronJ IT Technologies Private Limited</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nResponsibilities and Duties\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Opportunity For Data Scientist Internship - Be...</td>\n",
       "      <td>Corner Stone Solutions</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nLocation - Bangalore / Bengal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist/ Analyst</td>\n",
       "      <td>Becton Dickinson India Pvt. Ltd</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nRoles and Responsibilities\\no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist - Machine Learning</td>\n",
       "      <td>AugmatrixGo</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nRoles and Responsibilities\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist || Data Analyst || Data science</td>\n",
       "      <td>Inspiration Manpower Consultancy Pvt. Ltd.</td>\n",
       "      <td>Navi Mumbai, Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nJob description\\nJob Summary ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DBCG IND - GAMMA Senior Data Scientist</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>Mumbai, New Delhi, Chennai, Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\n    What Youll Do\\n\\nWe re lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist/Senior Data Scientist</td>\n",
       "      <td>GANIT BUSINESS SOLUTIONS PRIVATE LIMITED</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Chennai, Bangalo...</td>\n",
       "      <td>Job description\\n\\nAbout Ganit Inc\\n\\nFounded ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Scientist | CES IT LTD | CMMI Level 5</td>\n",
       "      <td>CES Ltd.</td>\n",
       "      <td>Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...</td>\n",
       "      <td>Job description\\nRoles and Responsibilities\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Global Medical Data Scientist</td>\n",
       "      <td>GlaxoSmithKline Pharmaceuticals Limited</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>This is an ideal role for an experienced candi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Associate Data Scientist - CRM &amp; Loyalty</td>\n",
       "      <td>Shell India Markets Private Limited</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nThe Role\\nGeneral Position De...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0                                     Data Scientist   \n",
       "1  Opportunity For Data Scientist Internship - Be...   \n",
       "2                            Data Scientist/ Analyst   \n",
       "3                  Data Scientist - Machine Learning   \n",
       "4     Data Scientist || Data Analyst || Data science   \n",
       "5             DBCG IND - GAMMA Senior Data Scientist   \n",
       "6               Data Scientist/Senior Data Scientist   \n",
       "7  Senior Data Scientist | CES IT LTD | CMMI Level 5   \n",
       "8                      Global Medical Data Scientist   \n",
       "9           Associate Data Scientist - CRM & Loyalty   \n",
       "\n",
       "                                      Company  \\\n",
       "0       CronJ IT Technologies Private Limited   \n",
       "1                      Corner Stone Solutions   \n",
       "2             Becton Dickinson India Pvt. Ltd   \n",
       "3                                 AugmatrixGo   \n",
       "4  Inspiration Manpower Consultancy Pvt. Ltd.   \n",
       "5                     Boston Consulting Group   \n",
       "6    GANIT BUSINESS SOLUTIONS PRIVATE LIMITED   \n",
       "7                                    CES Ltd.   \n",
       "8     GlaxoSmithKline Pharmaceuticals Limited   \n",
       "9         Shell India Markets Private Limited   \n",
       "\n",
       "                                            Location  \\\n",
       "0                                Bangalore/Bengaluru   \n",
       "1                                Bangalore/Bengaluru   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                   Navi Mumbai, Bangalore/Bengaluru   \n",
       "5    Mumbai, New Delhi, Chennai, Bangalore/Bengaluru   \n",
       "6  Hyderabad/Secunderabad, Pune, Chennai, Bangalo...   \n",
       "7  Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                                    Full Description  \n",
       "0  Job description\\nResponsibilities and Duties\\n...  \n",
       "1  Job description\\nLocation - Bangalore / Bengal...  \n",
       "2  Job description\\nRoles and Responsibilities\\no...  \n",
       "3  Job description\\nRoles and Responsibilities\\n\\...  \n",
       "4  Job description\\nJob description\\nJob Summary ...  \n",
       "5  Job description\\n    What Youll Do\\n\\nWe re lo...  \n",
       "6  Job description\\n\\nAbout Ganit Inc\\n\\nFounded ...  \n",
       "7  Job description\\nRoles and Responsibilities\\n\\...  \n",
       "8  This is an ideal role for an experienced candi...  \n",
       "9  Job description\\nThe Role\\nGeneral Position De...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the driver instance for the search keyword and filters\n",
    "d, dataFrame = get_highlevel_details_naukri('Data Scientist', location='Bangalore', stop=10)\n",
    "\n",
    "#pass the driver instance to get the full descriptions as a list and create a new column with it\n",
    "d, dataFrame['Full Description'] = get_full_descrp_naukri(d, 10)\n",
    "\n",
    "#quit the driver instance\n",
    "d.quit()\n",
    "\n",
    "#dropping unwanted columns\n",
    "dataFrame.drop(['Experience Required', 'Salary offered'], axis=1, inplace = True)\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Job description\\nRoles and Responsibilities\\nob Description Summary:\\nAs a member of the BD Advanced Analytics team, the Data Scientist role will collect, interpret, and deploy predictive analytics and machine learning models to assist our business partners in delivering advanced data analytics solutions to address key strategic business initiatives. The primary focus being the development and deployment of actionable insights that have a positive impact on BD’s business operations.\\nJob Description:\\nEvaluating business requirements and developing compelling user stories in collaboration with business stakeholders across various functions and regions\\nCollecting, interpreting, preparing and modelling data to provide actionable insights\\nPrototyping advanced data analytics solutions for presentation to business partners and stakeholders\\nApplying predictive analytics and machine learning methods and techniques appropriately to address business requirements\\nProvide expertise to guide business stakeholders in identifying opportunities for the use of BD’s data assets for the purpose of advanced data analytics\\nWork closely with data engineering and DataOps peers to productionalize advanced data analytics use cases\\nQualifications\\nMaster’s degree in STEM field or equivalent demonstrated work experience\\nExperience with Python, R, Hadoop, HIVE, SPARK, Jupyter and related advanced analytics / machine learning libraries (Scikitlearn, Tensorflow, Keras, )\\nMinimum 2 years of relevant work experience\\nAbility to translate complex solutions to a non-technical audience (storytelling & visualization)\\nExpertise in machine learning, neural networks, clustering, classification, regression and other common advanced analytics methods and techniques\\nMature competency for critical thinking, problem solving, working with ambiguity, communications and collaboration\\nPreferred\\nExperience with technologies related to data preparation, processing & optimization (e.g. Azure, HQL, SPARK SQL)\\nExperience with data visualization tools (Power BI)\\nFlexibility to accommodate meetings with international stakeholders in their respective time-zone\\n\\n\\n\\nRoleSoftware Developer\\nIndustry TypeIT-Software, Software Services\\nFunctional AreaIT Software - eCommerce, Internet Technologies\\nEmployment TypeFull Time, Permanent\\nRole CategoryProgramming & Design\\nEducation\\nUG :B.Tech/B.E. in Any Specialization\\nPG :Any Postgraduate in Any Specialization\\nDoctorate :Doctorate Not Required\\nKey Skills\\nRHiveHadoopData AnalyticsMachine LearningPythonPredictive Analytics'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking a full description\n",
    "dataFrame.loc[2, 'Full Description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You have to use the location and salary filter.\n",
    "2. You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "3. You have to scrape the job-title, job-location, company_name, experience_required.\n",
    "\n",
    "1. The location filter to be used is “Delhi/NCR”\n",
    "2. The salary filter to be used is “3-6” lakhs\n",
    "\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field .WEB SCRAPING ASSIGNMENT-2.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note- All of the above steps have to be done in code. No step is to be done \n",
    "manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scientist Jobs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience Required</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Developer - Data Science</td>\n",
       "      <td>ICL Systems India Private Limited</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "      <td>Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist/Data Analyst - Python/Machine L...</td>\n",
       "      <td>Change leaders</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Mumbai, Ghaziabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Amity University</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "      <td>Ghaziabad, Faridabad, Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist - Python &amp; Machine Learning</td>\n",
       "      <td>FUTURES AND CAREERS</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Bangalore/Bengal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist - Python &amp; Machine Learning</td>\n",
       "      <td>FUTURES AND CAREERS</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Chennai, Bangalo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist - Python / Machine Learning / T...</td>\n",
       "      <td>FUTURES AND CAREERS</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Bangalore/Beng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist - Python &amp; Machine Learning</td>\n",
       "      <td>FUTURES AND CAREERS</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Hyderabad/Secunderabad, Bangalore/Bengaluru, D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hiring For Data Scientist</td>\n",
       "      <td>Max Bupa Health Insurance Company Limited</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram, Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Required- Data Scientist (NLP)-Axis Bank - 6 m...</td>\n",
       "      <td>Axis Bank Limited</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "      <td>Kolkata, New Delhi, Hyderabad/Secunderabad, Pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist - Python / Machine Learning / T...</td>\n",
       "      <td>FUTURES AND CAREERS</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Hyderabad/Secunderabad, Bangalore/Bengaluru, D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0                           Developer - Data Science   \n",
       "1  Data Scientist/Data Analyst - Python/Machine L...   \n",
       "2                                     Data Scientist   \n",
       "3         Data Scientist - Python & Machine Learning   \n",
       "4         Data Scientist - Python & Machine Learning   \n",
       "5  Data Scientist - Python / Machine Learning / T...   \n",
       "6         Data Scientist - Python & Machine Learning   \n",
       "7                          Hiring For Data Scientist   \n",
       "8  Required- Data Scientist (NLP)-Axis Bank - 6 m...   \n",
       "9  Data Scientist - Python / Machine Learning / T...   \n",
       "\n",
       "                                     Company Experience Required  \\\n",
       "0          ICL Systems India Private Limited             3-5 Yrs   \n",
       "1                             Change leaders            5-10 Yrs   \n",
       "2                           Amity University             6-8 Yrs   \n",
       "3                        FUTURES AND CAREERS             2-7 Yrs   \n",
       "4                        FUTURES AND CAREERS             2-7 Yrs   \n",
       "5                        FUTURES AND CAREERS             3-8 Yrs   \n",
       "6                        FUTURES AND CAREERS             2-7 Yrs   \n",
       "7  Max Bupa Health Insurance Company Limited             1-6 Yrs   \n",
       "8                          Axis Bank Limited             4-9 Yrs   \n",
       "9                        FUTURES AND CAREERS             3-8 Yrs   \n",
       "\n",
       "                                            Location  \n",
       "0                                        Delhi / NCR  \n",
       "1                                  Mumbai, Ghaziabad  \n",
       "2                  Ghaziabad, Faridabad, Delhi / NCR  \n",
       "3  Hyderabad/Secunderabad, Pune, Bangalore/Bengal...  \n",
       "4  Hyderabad/Secunderabad, Pune, Chennai, Bangalo...  \n",
       "5  Mumbai, Hyderabad/Secunderabad, Bangalore/Beng...  \n",
       "6  Hyderabad/Secunderabad, Bangalore/Bengaluru, D...  \n",
       "7                      Gurgaon/Gurugram, Delhi / NCR  \n",
       "8  Kolkata, New Delhi, Hyderabad/Secunderabad, Pu...  \n",
       "9  Hyderabad/Secunderabad, Bangalore/Bengaluru, D...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using the 'get_highlevel_details_naukri' function to fetch the results from naukri website\n",
    "driver,dataFrame=get_highlevel_details_naukri('Data Scientist', location = 'Delhi / NCR', salary_range='3-6 Lakhs', stop=10, use_filter=True)\n",
    "driver.quit()\n",
    "\n",
    "#dropping unwanted column\n",
    "dataFrame.drop(['Salary offered'],axis=1,inplace=True)\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "2. Enter “Data Scientist” in “Job Title,Keyword,Company” field and enter “Noida” in “location” field.\n",
    "3. Then click the search button. You will land up in the below page:\n",
    "4. Then scrape the data for the first 10 jobs results you get in the above shown page.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note- All of the above steps have to be done in code. No step is to be done\n",
    "manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes Glassdoor requires the user to log in in order to view the pages. Hence created a log in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_glassdoor(driver):\n",
    "    \"\"\"\n",
    "    This function fetches the user name and password from a config.ini file and logs in into Glassdoor website\n",
    "    \"\"\"\n",
    "    #Loading the config file\n",
    "    config = ConfigParser()\n",
    "    config.read('./config.ini')\n",
    "    \n",
    "    #getting the username and password\n",
    "    username = config.get('credentials','user_name')\n",
    "    paswd = config.get('credentials','password')\n",
    "    \n",
    "    #signing in as a valid user\n",
    "    main_window = driver.current_window_handle\n",
    "    try:\n",
    "        driver.find_element_by_class_name(\"locked-home-sign-in\").click()\n",
    "    except NoSuchElementException:\n",
    "        driver.find_element_by_xpath(\"//li[@class='sign-in']/a\").click()\n",
    "    time.sleep(3)\n",
    "    driver.find_element_by_id(\"userEmail\").send_keys(username)\n",
    "    driver.find_element_by_id(\"userPassword\").send_keys(paswd)\n",
    "    driver.find_element_by_xpath(\"//button[contains(text(),'Sign In')]\").click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    driver.switch_to.window(main_window)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highlevel_details_glassdoor(job, location = None, salary_range=None, stop=None, use_filter=False):\n",
    "    \"\"\"\n",
    "    This function gets the job title, location, salary_range, number of rows as input and fetches the job titles, companies, \n",
    "    age of the job post, locations and ratings data as a dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    titles=[]\n",
    "    companies=[]\n",
    "    no_days_posts=[]\n",
    "    #salaries=[]\n",
    "    locs=[]\n",
    "    ratings=[]\n",
    "\n",
    "    #Glassdoor url\n",
    "    url = 'https://www.glassdoor.co.in/index.htm'\n",
    "    #launching the website.\n",
    "    drivr = get_drive_launch(url)\n",
    "    #logging in into Glassdoor site\n",
    "    drivr = login_glassdoor(drivr)\n",
    "    \n",
    "    #entering the search keyword\n",
    "    drivr.find_element_by_id(\"sc.keyword\").send_keys(job)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    #Entering the location details\n",
    "    if ((location) and (not use_filter)):\n",
    "        drivr.find_element_by_xpath(\"//input[@data-test='search-bar-location-input']\").send_keys(Keys.CONTROL + \"a\")\n",
    "        drivr.find_element_by_xpath(\"//input[@data-test='search-bar-location-input']\").send_keys(Keys.DELETE)\n",
    "        drivr.find_element_by_xpath(\"//input[@data-test='search-bar-location-input']\").send_keys(location)\n",
    "    time.sleep(2)\n",
    "    drivr.find_element_by_xpath(\"//button[@type='submit']\").click()\n",
    "    \n",
    "    #Entering location in filter facet\n",
    "    if use_filter:        \n",
    "        if location:\n",
    "            time.sleep(3)\n",
    "            #Expanding and finding the location\n",
    "            try:\n",
    "                drivr.find_element_by_xpath(\"//span[@class = 'ellipsis fleft' and contains(text(), \"+\"'\"+location+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "            except:\n",
    "                drivr.find_element_by_xpath(\"//div[@data-filter-id='citiesGid']//a\").click()\n",
    "                time.sleep(3)\n",
    "                drivr.find_element_by_xpath(\"//span[@class = 'ellipsis fleft' and contains(text(), \"+\"'\"+location+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "        \n",
    "        #entering the salary range as a filter\n",
    "        if salary_range:\n",
    "\n",
    "            try:\n",
    "                drivr.find_element_by_xpath(\"//span[contains(text(), \"+\"'\"+salary_range+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "            except:\n",
    "                drivr.find_element_by_xpath(\"//div[@data-filter-id='salaryRange']//a\").click()\n",
    "                time.sleep(3)\n",
    "                drivr.find_element_by_xpath(\"//span[contains(text(), \"+\"'\"+salary_range+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "\n",
    "    time.sleep(5)\n",
    "    \n",
    "    #Printing the search result topic\n",
    "    print(drivr.find_element_by_xpath(\"//h1[@data-test='jobTitle']\").text)\n",
    "    \n",
    "    #Fetching the job tiles as web elements\n",
    "    job_links = drivr.find_elements_by_xpath(\"//div[@class='d-flex flex-column css-x75kgh e1rrn5ka3']\")\n",
    "    \n",
    "    #Fetching ratings\n",
    "    for job_link in job_links[:stop]:\n",
    "        try:\n",
    "            if job_link.text:\n",
    "                ratings.append(job_link.text)\n",
    "            else:\n",
    "                ratings.append(\"No Rating\")\n",
    "            #ratings.append(job_link.find_element_by_xpath(\"//span[@class='css-19pjha7 e1cjmv6j1']\").text)\n",
    "        except:\n",
    "            ratings.append(\"No Rating\")\n",
    "    \n",
    "    #Fetching the web elements\n",
    "    job_titles = drivr.find_elements_by_xpath(\"//a[@data-test='job-link']/span\")\n",
    "    job_companies = drivr.find_elements_by_xpath(\"//div[@class='d-flex justify-content-between align-items-start']//a/span\")\n",
    "    job_days = drivr.find_elements_by_xpath(\"//div[@data-test='job-age']\")\n",
    "    #job_salaries = drivr.find_elements_by_xpath(\"//div[@class='info fleft']/ul/li[2]\")\n",
    "    job_locations = drivr.find_elements_by_xpath(\"//div[@class='d-flex flex-wrap css-11d3uq0 e1rrn5ka2']/span\")\n",
    "    \n",
    "    #Fetching texts from the webelements\n",
    "    for job_title, job_company, job_day, job_loc in zip(job_titles[:stop],job_companies[:stop],job_days[:stop],job_locations[:stop]):\n",
    "        titles.append(job_title.text.strip())\n",
    "        companies.append(job_company.text.strip())\n",
    "        no_days_posts.append(job_day.text.strip())\n",
    "        #salaries.append(job_salary.text)\n",
    "        locs.append(job_loc.text.strip())\n",
    "    \n",
    "    #Returning the driver instance and the dataframe\n",
    "    return drivr, pd.DataFrame({'Job Title':titles, 'Company':companies, 'Job Post Age':no_days_posts, 'Location':locs, 'Ratings':ratings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scientist Jobs in Noida\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Job Post Age</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Salasar New Age Technologies</td>\n",
       "      <td>30d+</td>\n",
       "      <td>No Rating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Biz2Credit Inc</td>\n",
       "      <td>30d+</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Techlive</td>\n",
       "      <td>30d+</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adobe</td>\n",
       "      <td>6d</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SearchUrCollege</td>\n",
       "      <td>30d+</td>\n",
       "      <td>No Rating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CRMNEXT</td>\n",
       "      <td>12d</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Microsoft</td>\n",
       "      <td>30d+</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Salasar New Age Technologies</td>\n",
       "      <td>30d+</td>\n",
       "      <td>No Rating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Unyscape Infocom Pvt. Ltd</td>\n",
       "      <td>30d+</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Genpact</td>\n",
       "      <td>1d</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Company Job Post Age    Ratings\n",
       "0  Salasar New Age Technologies         30d+  No Rating\n",
       "1                Biz2Credit Inc         30d+        3.8\n",
       "2                      Techlive         30d+        5.0\n",
       "3                         Adobe           6d        4.4\n",
       "4               SearchUrCollege         30d+  No Rating\n",
       "5                       CRMNEXT          12d        3.5\n",
       "6                     Microsoft         30d+        4.4\n",
       "7  Salasar New Age Technologies         30d+  No Rating\n",
       "8     Unyscape Infocom Pvt. Ltd         30d+        4.1\n",
       "9                       Genpact           1d        3.8"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping data\n",
    "driver, dataframe = get_highlevel_details_glassdoor('Data Scientist','Noida',stop=10)\n",
    "driver.quit()\n",
    "\n",
    "#Selecting columns\n",
    "dataframe=dataframe[['Company', 'Job Post Age', 'Ratings']]\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location. You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above task will be, done as shown in the below steps:\n",
    "1. first get the webpage https://www.glassdoor.co.in/Salaries/index.htm\n",
    "2. Enter “Data Scientist” in Job title field and “Noida” in location field.\n",
    "3. Click the search button.\n",
    "4. After that you will land on the below page You have to scrape whole data from this webpage\n",
    "5. Scrape data for first 10 companies. Scrape the min salary, max salary, company name, Average salary and rating of the company.\n",
    "6. Store the data in a dataframe.\n",
    "\n",
    "Note that all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_salary_details_glassdoor(url, job_title, location, stop=None):\n",
    "    \"\"\"\n",
    "    This function will get the url, job title, location and number of rows(stop) as input and fetches company, minimum salary, \n",
    "    average salary, maximum salary and number of salaries as a dataframe output.\n",
    "    \"\"\"\n",
    "    companies = []\n",
    "    min_salaries = []\n",
    "    max_salaries = []\n",
    "    avg_salaries = []\n",
    "    num_salaries = []\n",
    "    \n",
    "    #Launch the site\n",
    "    drivr = get_drive_launch(url)\n",
    "    \n",
    "    #log in into the site\n",
    "    drivr = login_glassdoor(drivr)\n",
    "    \n",
    "    #Entering the search keyword and location\n",
    "    drivr.find_element_by_id(\"KeywordSearch\").send_keys(job_title)\n",
    "    drivr.find_element_by_id(\"LocationSearch\").send_keys(Keys.CONTROL + \"a\")\n",
    "    drivr.find_element_by_id(\"LocationSearch\").send_keys(Keys.DELETE)\n",
    "    drivr.find_element_by_id(\"LocationSearch\").send_keys(location)\n",
    "    drivr.find_element_by_id(\"HeroSearchButton\").click()\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "    limit = stop\n",
    "    \n",
    "    #Looping till we reach the number of rows requested by user\n",
    "    while len(companies) < limit:\n",
    "        #Get the web elements\n",
    "        company_elems = drivr.find_elements_by_xpath(\"//div[@data-test='job-info']/p[2]\")\n",
    "        num_sal_elems = drivr.find_elements_by_xpath(\"//div[@data-test='job-info']/p[@class='css-1uyte9r css-1kuy7z7 m-0 ']\")\n",
    "        avg_sal_elems = drivr.find_elements_by_xpath(\"//div[@class='col-2 d-none d-md-flex flex-row justify-content-end']\")\n",
    "        min_sal_elems = drivr.find_elements_by_xpath(\"//div[@class='common__RangeBarStyle__values d-flex justify-content-between ']/span[1]\")\n",
    "        max_sal_elems = drivr.find_elements_by_xpath(\"//div[@class='common__RangeBarStyle__values d-flex justify-content-between ']/span[2]\")\n",
    "        \n",
    "        #Loop over the web elements and fetch the data\n",
    "        for company, num_sal, avg_sal, min_sal, max_sal in zip(company_elems, num_sal_elems, avg_sal_elems, min_sal_elems, max_sal_elems):\n",
    "            companies.append(company.text)\n",
    "            num_salaries.append(num_sal.text)\n",
    "            avg_salaries.append(avg_sal.text.replace('\\n',''))\n",
    "            min_salaries.append(min_sal.text)\n",
    "            max_salaries.append(max_sal.text)\n",
    "            \n",
    "            #Break if the requested number of rows are achieved within for loop\n",
    "            if len(companies) == limit:\n",
    "                break\n",
    "        #Break if the requested number of rows are achieved within while loop\n",
    "        if len(companies) == limit:\n",
    "                break\n",
    "        \n",
    "        #Get the url to the next page if number of rows requested is not achieved yet\n",
    "        url = drivr.find_element_by_xpath(\"//a[@class='pagination__ArrowStyle__nextArrow  ']\").get_attribute('href')\n",
    "        \n",
    "        #quit the browser driver and launch a new driver with the url to the next page and repeat the steps above\n",
    "        drivr.quit()\n",
    "        drivr = get_drive_launch(url)\n",
    "        time.sleep(2)\n",
    "    \n",
    "    #Create an output dataframe with the data fetched and return\n",
    "    dataframe = pd.DataFrame({'Company': companies, 'Number of Salaries': num_salaries, 'Average Salary': avg_salaries, 'Minimum Salary': min_salaries, 'Maximum Salary': max_salaries})\n",
    "    return drivr, dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Number of Salaries</th>\n",
       "      <th>Average Salary</th>\n",
       "      <th>Minimum Salary</th>\n",
       "      <th>Maximum Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tata Consultancy Services</td>\n",
       "      <td>14 salaries</td>\n",
       "      <td>₹ 6,01,000/yr</td>\n",
       "      <td>₹336L</td>\n",
       "      <td>₹1,080L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>14 salaries</td>\n",
       "      <td>₹ 11,51,207/yr</td>\n",
       "      <td>₹579L</td>\n",
       "      <td>₹2,222L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Delhivery</td>\n",
       "      <td>14 salaries</td>\n",
       "      <td>₹ 12,34,207/yr</td>\n",
       "      <td>₹452L</td>\n",
       "      <td>₹11,669L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IBM</td>\n",
       "      <td>13 salaries</td>\n",
       "      <td>₹ 7,63,825/yr</td>\n",
       "      <td>₹589L</td>\n",
       "      <td>₹2,741L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ericsson-Worldwide</td>\n",
       "      <td>12 salaries</td>\n",
       "      <td>₹ 7,32,209/yr</td>\n",
       "      <td>₹350L</td>\n",
       "      <td>₹1,619L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>10 salaries</td>\n",
       "      <td>₹ 13,88,910/yr</td>\n",
       "      <td>₹1,050L</td>\n",
       "      <td>₹1,500L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Valiance Solutions</td>\n",
       "      <td>9 salaries</td>\n",
       "      <td>₹ 8,18,515/yr</td>\n",
       "      <td>₹504L</td>\n",
       "      <td>₹1,471L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Innovaccer</td>\n",
       "      <td>8 salaries</td>\n",
       "      <td>₹ 12,01,403/yr</td>\n",
       "      <td>₹623L</td>\n",
       "      <td>₹1,702L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ZS Associates</td>\n",
       "      <td>7 salaries</td>\n",
       "      <td>₹ 10,00,000/yr</td>\n",
       "      <td>₹203L</td>\n",
       "      <td>₹1,817L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EXL Service</td>\n",
       "      <td>7 salaries</td>\n",
       "      <td>₹ 11,90,000/yr</td>\n",
       "      <td>₹578L</td>\n",
       "      <td>₹1,500L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Company Number of Salaries  Average Salary  \\\n",
       "0  Tata Consultancy Services        14 salaries   ₹ 6,01,000/yr   \n",
       "1                  Accenture        14 salaries  ₹ 11,51,207/yr   \n",
       "2                  Delhivery        14 salaries  ₹ 12,34,207/yr   \n",
       "3                        IBM        13 salaries   ₹ 7,63,825/yr   \n",
       "4         Ericsson-Worldwide        12 salaries   ₹ 7,32,209/yr   \n",
       "5         UnitedHealth Group        10 salaries  ₹ 13,88,910/yr   \n",
       "6         Valiance Solutions         9 salaries   ₹ 8,18,515/yr   \n",
       "7                 Innovaccer         8 salaries  ₹ 12,01,403/yr   \n",
       "8              ZS Associates         7 salaries  ₹ 10,00,000/yr   \n",
       "9                EXL Service         7 salaries  ₹ 11,90,000/yr   \n",
       "\n",
       "  Minimum Salary Maximum Salary  \n",
       "0          ₹336L        ₹1,080L  \n",
       "1          ₹579L        ₹2,222L  \n",
       "2          ₹452L       ₹11,669L  \n",
       "3          ₹589L        ₹2,741L  \n",
       "4          ₹350L        ₹1,619L  \n",
       "5        ₹1,050L        ₹1,500L  \n",
       "6          ₹504L        ₹1,471L  \n",
       "7          ₹623L        ₹1,702L  \n",
       "8          ₹203L        ₹1,817L  \n",
       "9          ₹578L        ₹1,500L  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass the url and fetch the data\n",
    "url = 'https://www.glassdoor.co.in/Salaries/index.htm'\n",
    "d, df = get_salary_details_glassdoor(url, 'Data Scientist', 'Noida', 10)\n",
    "#Quit the driver\n",
    "d.quit()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6 : Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount %\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape the data you have to go through following steps:\n",
    "1. Go to flipkart webpage by url https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon\n",
    "3. after that you will reach to a webpage having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "4. after scraping data from the first page, go to the “Next” Button at the bottom of the page , then click on it\n",
    "5. Now scrape data from this page as usual\n",
    "6. repeat this until you get data for 100 sunglasses.\n",
    "\n",
    "Note that all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_highlevel_details_flipkart(product_name, no_of_products = 100):\n",
    "    \"\"\"\n",
    "    This function gets the product name and number of products to be fetched from flipkart and gives the highlevel data like\n",
    "    Product name, product short description, Product price and product discounts as a dataframe. \n",
    "    \"\"\"\n",
    "    \n",
    "    product_names = []\n",
    "    product_descriptions = []\n",
    "    product_prices = []\n",
    "    product_discounts = []\n",
    "    \n",
    "    #launch the flipkart site\n",
    "    url = 'https://www.flipkart.com/'\n",
    "    driver = get_drive_launch(url)\n",
    "    driver.find_element_by_xpath(\"//button[contains(text(),'✕')]\").click()\n",
    "\n",
    "    #searching the product\n",
    "    driver.find_element_by_xpath(\"//input[@title='Search for products, brands and more']\").send_keys(product_name)\n",
    "    driver.find_element_by_xpath(\"//button[@type='submit']\").click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    #Looping until the requested number of products are fetched\n",
    "    while len(product_names) < no_of_products:\n",
    "        #Getting the web elements\n",
    "        product_name_elems = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "        product_description_elems = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']/../a[not(@class='_3bPFwb')]\")\n",
    "        price_elems= driver.find_elements_by_xpath(\"//a[@class='_3bPFwb']//div[@class='_30jeq3']\")\n",
    "        price_discount_elems = driver.find_elements_by_xpath(\"//a[@class='_3bPFwb']\")\n",
    "        \n",
    "        #Looping over the web elements and fetching the texts\n",
    "        for prod_name_elem, product_description_elem, price_elem, price_discount_elem in zip(product_name_elems,product_description_elems,price_elems,price_discount_elems):\n",
    "            product_names.append(prod_name_elem.text.strip())\n",
    "            product_descriptions.append(product_description_elem.text.strip())\n",
    "            product_prices.append(price_elem.text.strip())\n",
    "            try:\n",
    "                discount = price_discount_elem.find_element_by_class_name(\"_3Ay6Sb\").text\n",
    "            except:\n",
    "                discount = None\n",
    "            if discount:\n",
    "                product_discounts.append(discount)\n",
    "            else:\n",
    "                product_discounts.append(\"0% off\")\n",
    "            #Break if number of products requested is obtained within for loop\n",
    "            if len(product_names) == no_of_products:\n",
    "                break\n",
    "        \n",
    "        #Break if number of products requested is obtained within While loop\n",
    "        if len(product_names) == no_of_products:\n",
    "            break\n",
    "        #Go to the next page url if more products are required\n",
    "        driver.find_element_by_xpath(\"//span[contains(text(),'Next')]\").click()\n",
    "        time.sleep(3)\n",
    "    #Create a dataframe and return driver instance and the dataframe\n",
    "    dataframe = pd.DataFrame({'Product Brand': product_names, 'Product Description' : product_descriptions, 'Product Price' : product_prices,'Product Discount':product_discounts})\n",
    "    return driver, dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Product Price</th>\n",
       "      <th>Product Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹758</td>\n",
       "      <td>15% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹695</td>\n",
       "      <td>13% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Mirrored, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (Free Size)</td>\n",
       "      <td>₹314</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹225</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>NuVew</td>\n",
       "      <td>UV Protection, Gradient, Night Vision, Mirrore...</td>\n",
       "      <td>₹377</td>\n",
       "      <td>69% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Round Sunglasses (52)</td>\n",
       "      <td>₹1,243</td>\n",
       "      <td>4% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Round Sunglasses (Free...</td>\n",
       "      <td>₹426</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>FLYING MACHINE</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹686</td>\n",
       "      <td>57% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Gradient, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹757</td>\n",
       "      <td>5% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Product Brand                                Product Description  \\\n",
       "0         Fastrack      UV Protection Wayfarer Sunglasses (Free Size)   \n",
       "1         Fastrack   UV Protection Rectangular Sunglasses (Free Size)   \n",
       "2         Fastrack  Mirrored, UV Protection Wayfarer Sunglasses (F...   \n",
       "3           PIRASO       UV Protection Aviator Sunglasses (Free Size)   \n",
       "4           PIRASO              UV Protection Aviator Sunglasses (54)   \n",
       "..             ...                                                ...   \n",
       "95           NuVew  UV Protection, Gradient, Night Vision, Mirrore...   \n",
       "96        Fastrack                UV Protection Round Sunglasses (52)   \n",
       "97  ROZZETTA CRAFT  UV Protection, Gradient Round Sunglasses (Free...   \n",
       "98  FLYING MACHINE      UV Protection Wayfarer Sunglasses (Free Size)   \n",
       "99        Fastrack  Gradient, UV Protection Wayfarer Sunglasses (F...   \n",
       "\n",
       "   Product Price Product Discount  \n",
       "0           ₹758          15% off  \n",
       "1           ₹695          13% off  \n",
       "2           ₹499          50% off  \n",
       "3           ₹314          80% off  \n",
       "4           ₹225          85% off  \n",
       "..           ...              ...  \n",
       "95          ₹377          69% off  \n",
       "96        ₹1,243           4% off  \n",
       "97          ₹426          84% off  \n",
       "98          ₹686          57% off  \n",
       "99          ₹757           5% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fetching data\n",
    "d, dataFrame = get_product_highlevel_details_flipkart('sunglasses', 100)\n",
    "\n",
    "d.quit()\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes\u0002earpods-power\u0002adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you will open the above link you will reach to the below shown webpage.\n",
    "\n",
    "As shown in the above page you have to scrape the tick marked attributes.\n",
    "These are \n",
    "1. Rating \n",
    "2. Review_summary \n",
    "3. Full review\n",
    "\n",
    "You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews_flipkart(product_url, no_of_reviews):\n",
    "    \"\"\"\n",
    "    This function gets the product details page url and number of reviews as input and give the Full review of each product \n",
    "    page urls passed as input.\n",
    "    \"\"\"\n",
    "\n",
    "    ratings = []\n",
    "    review_summaries = []\n",
    "    full_reviews = []\n",
    "    \n",
    "    #launch the webpage\n",
    "    drivr = get_drive_launch(product_url)\n",
    "    #Get the reviews url from the page\n",
    "    product_reviews_url = drivr.find_element_by_xpath(\"//div[@class='_3UAT2v _16PBlm']/..\").get_attribute('href')\n",
    "    #quit the driver and launch the review page using the above url fetched from the product page\n",
    "    drivr.quit()\n",
    "    drivr = get_drive_launch(product_reviews_url)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    #Run a while loop until the numbr of reviews requested are obtained\n",
    "    while len(ratings) < no_of_reviews:\n",
    "        #get the webelements\n",
    "        rating_elems = drivr.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "        review_summary_elems = drivr.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "        full_review_elems= drivr.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "        \n",
    "        #loop over the list of webelements to fetch the text data\n",
    "        for rating_elem,review_summary_elem,full_review_elem in zip(rating_elems,review_summary_elems,full_review_elems):\n",
    "            ratings.append(rating_elem.text.strip())\n",
    "            review_summaries.append(review_summary_elem.text.strip())\n",
    "            full_reviews.append(full_review_elem.text.strip())\n",
    "            #Break if number of reviews requested is obtained within for loop\n",
    "            if len(ratings) == no_of_reviews:\n",
    "                break\n",
    "        #Break if number of reviews requested is obtained within while loop\n",
    "        if len(ratings) == no_of_reviews:\n",
    "            break\n",
    "        #Go to the next page if number of reviews are not met yet\n",
    "        drivr.find_element_by_xpath(\"//span[contains(text(),'Next')]\").click()\n",
    "        time.sleep(3)\n",
    "    #create and return the dataframe output and driver instance\n",
    "    dataframe = pd.DataFrame({'Rating': ratings, 'Review Summary' : review_summaries, 'Full Review' : full_reviews})\n",
    "    return drivr, dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Summary</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the Money\\n\\nThe iPhone 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.\\n\\nI’m am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>iphone 11 is a very good phone to buy only if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Super!</td>\n",
       "      <td>I’d like to start by saying that the overall e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific</td>\n",
       "      <td>Have used both iPhone X and iPhone XR and I ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>4</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>impressive super phone and best in class camer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific</td>\n",
       "      <td>Good buy.. working perfectly !\\n\\nThat was upg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Best and amazing product.....phone looks so pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating      Review Summary  \\\n",
       "0       5           Brilliant   \n",
       "1       5    Perfect product!   \n",
       "2       5   Worth every penny   \n",
       "3       5       Great product   \n",
       "4       5  Highly recommended   \n",
       "..    ...                 ...   \n",
       "95      5              Super!   \n",
       "96      5            Terrific   \n",
       "97      4           Very Good   \n",
       "98      5            Terrific   \n",
       "99      5    Perfect product!   \n",
       "\n",
       "                                          Full Review  \n",
       "0   The Best Phone for the Money\\n\\nThe iPhone 11 ...  \n",
       "1   Amazing phone with great cameras and better ba...  \n",
       "2   Previously I was using one plus 3t it was a gr...  \n",
       "3   Amazing Powerful and Durable Gadget.\\n\\nI’m am...  \n",
       "4   iphone 11 is a very good phone to buy only if ...  \n",
       "..                                                ...  \n",
       "95  I’d like to start by saying that the overall e...  \n",
       "96  Have used both iPhone X and iPhone XR and I ca...  \n",
       "97  impressive super phone and best in class camer...  \n",
       "98  Good buy.. working perfectly !\\n\\nThat was upg...  \n",
       "99  Best and amazing product.....phone looks so pr...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_url = 'https://www.flipkart.com/apple-iphone-11-black-64-gb-includes%02earpods-power%02adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace'\n",
    "d, DataFrame = get_reviews_flipkart(product_url, 100)\n",
    "d.quit()\n",
    "DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amazing phone with great cameras and better battery which gives you the best performance. I just love the camera .'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verifying a full review\n",
    "DataFrame.loc[1,'Full Review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to scrape 4 attributes of each sneaker :\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. discount %\n",
    "\n",
    "As shown in the below image, you have to scrape the tick marked attributes\n",
    "\n",
    "Also note that all the steps required during scraping should be done through code \n",
    "only and not manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Product Price</th>\n",
       "      <th>Product Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French Connection</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹799</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ROCKFIELD</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹450</td>\n",
       "      <td>54% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Perfect &amp; Affordable Combo Pack of 02 Pairs Sn...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>72% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Robbie jones</td>\n",
       "      <td>Casual Sneakers Shoes For Men Sneakers For Men</td>\n",
       "      <td>₹399</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Combo Pack of 4 Casual Sneakers With Sneakers ...</td>\n",
       "      <td>₹474</td>\n",
       "      <td>76% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Englewood</td>\n",
       "      <td>White Shoes For Men | Casual White Laceups Sho...</td>\n",
       "      <td>₹496</td>\n",
       "      <td>66% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Men's Combo Pack of 02 Shoes for Men Casual Sn...</td>\n",
       "      <td>₹420</td>\n",
       "      <td>57% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Ktiz</td>\n",
       "      <td>Rockstyle Trending Multicolor Ultralight canva...</td>\n",
       "      <td>₹418</td>\n",
       "      <td>67% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ESSENCE</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹442</td>\n",
       "      <td>55% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ROCKFIELD</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹399</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product Brand                                Product Description  \\\n",
       "0   French Connection                                   Sneakers For Men   \n",
       "1           ROCKFIELD                                   Sneakers For Men   \n",
       "2              Chevit  Perfect & Affordable Combo Pack of 02 Pairs Sn...   \n",
       "3        Robbie jones     Casual Sneakers Shoes For Men Sneakers For Men   \n",
       "4              Chevit  Combo Pack of 4 Casual Sneakers With Sneakers ...   \n",
       "..                ...                                                ...   \n",
       "95          Englewood  White Shoes For Men | Casual White Laceups Sho...   \n",
       "96             Chevit  Men's Combo Pack of 02 Shoes for Men Casual Sn...   \n",
       "97               Ktiz  Rockstyle Trending Multicolor Ultralight canva...   \n",
       "98            ESSENCE                                   Sneakers For Men   \n",
       "99          ROCKFIELD                                   Sneakers For Men   \n",
       "\n",
       "   Product Price Product Discount  \n",
       "0           ₹799          60% off  \n",
       "1           ₹450          54% off  \n",
       "2           ₹499          72% off  \n",
       "3           ₹399          60% off  \n",
       "4           ₹474          76% off  \n",
       "..           ...              ...  \n",
       "95          ₹496          66% off  \n",
       "96          ₹420          57% off  \n",
       "97          ₹418          67% off  \n",
       "98          ₹442          55% off  \n",
       "99          ₹399          50% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reusing the same get_product_highlevel_details_flipkart(product_name, no_of_products) for this problem\n",
    "d, dataFrame = get_product_highlevel_details_flipkart('sneakers', 100)\n",
    "d.quit()\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9: Go to the link - https://www.myntra.com/shoes Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black” and then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe Please note that applying the filter and scraping the data , everything should be done through code only and there should not be any manual step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_details_myntra(url, num_of_items = 10, use_filter = False, price_range_item_num = 1, color = 'White'):\n",
    "    \"\"\"\n",
    "    This function fetch the product name, product short description and prices from Myntra website.\n",
    "    The function will take the following parameters as input:\n",
    "    url - Url of the website\n",
    "    num_of_items - number of items to fetch. default = 10\n",
    "    use_filter - whether to use the filter facet or not. default = False\n",
    "    price_range_item_num - The price range in the filter facet always has 4 options with dynamic range based on the product.\n",
    "                            This parameter uses the index to choose which filter to apply in price range.\n",
    "    color - the value to use in the 'color' filter facet\n",
    "    Returns:\n",
    "        driver instance\n",
    "        dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    product_names = []\n",
    "    product_short_desc = []\n",
    "    prices = []\n",
    "    \n",
    "    #launch the url\n",
    "    drivr = get_drive_launch(url)\n",
    "    \n",
    "    #select the filter values\n",
    "    if use_filter:\n",
    "        drivr.find_element_by_xpath(\"//ul[@class='price-list']/li[\"+str(price_range_item_num)+\"]\").click()\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            drivr.find_element_by_xpath(\"//li[@class='colour-listItem']/label[(contains(text(),'\"+color+\"'))]\").click()\n",
    "        except:\n",
    "            drivr.find_element_by_class_name(\"colour-more\").click()\n",
    "            drivr.find_element_by_xpath(\"//li[@class='colour-listItem']/label[(contains(text(),'\"+color+\"'))]\").click()\n",
    "        time.sleep(2)\n",
    "\n",
    "    #Loop until the number of items requested by the user is met\n",
    "    while (len(product_names) < num_of_items):  \n",
    "        #Get the web elements\n",
    "        brand_name_elems = drivr.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "        short_desc_elems = drivr.find_elements_by_xpath(\"//h4[@class='product-product']\")        \n",
    "        price_elems = drivr.find_elements_by_xpath(\"//div[@class='product-price']\")\n",
    "        \n",
    "        #loop over the list of webelements to fetch the text data\n",
    "        for brand_name, short_desc, price in zip(brand_name_elems, short_desc_elems, price_elems):\n",
    "            product_names.append(brand_name.text)\n",
    "            product_short_desc.append(short_desc.text)\n",
    "            prices.append(price.text.split('Rs. ')[1])\n",
    "            #Break if the number of items requested is met within the for look\n",
    "            if len(product_names) == num_of_items:\n",
    "                break\n",
    "        #Break if the number of items requested is met within the while look\n",
    "        if len(product_names) == num_of_items:\n",
    "                break\n",
    "        #Go to the next page if more items are needed\n",
    "        url = drivr.find_element_by_xpath(\"//a[@rel='next']\").get_attribute('href')\n",
    "        drivr.quit()\n",
    "        drivr = get_drive_launch(url)\n",
    "        time.sleep(3)\n",
    "    #Creat and return the output dataframe and driver instance\n",
    "    datafrm = pd.DataFrame({'Product Name':product_names, 'Product Short Description': product_short_desc, 'Price in INR':prices})\n",
    "    return drivr, datafrm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Product Short Description</th>\n",
       "      <th>Price in INR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men AIR ZOOM Running Shoes</td>\n",
       "      <td>11470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Men Fuse Training Sports Shoes</td>\n",
       "      <td>7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men KD13 EP Basketball Shoes</td>\n",
       "      <td>12995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men AIR MAX INFINITY 2 Sneaker</td>\n",
       "      <td>7050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men REACT MILER Running Shoes</td>\n",
       "      <td>9345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Women Leather Solid Pumps</td>\n",
       "      <td>11990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Women Solid Wedges</td>\n",
       "      <td>8490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hush Puppies</td>\n",
       "      <td>Men Solid Leather Formal Derbys</td>\n",
       "      <td>9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Heel &amp; Buckle London</td>\n",
       "      <td>Men Leather Formal Brogues</td>\n",
       "      <td>7693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Saint G</td>\n",
       "      <td>Women Leather Heeled Boots</td>\n",
       "      <td>9900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Product Name        Product Short Description Price in INR\n",
       "0                   Nike       Men AIR ZOOM Running Shoes        11470\n",
       "1                   Puma   Men Fuse Training Sports Shoes         7999\n",
       "2                   Nike     Men KD13 EP Basketball Shoes        12995\n",
       "3                   Nike   Men AIR MAX INFINITY 2 Sneaker         7050\n",
       "4                   Nike    Men REACT MILER Running Shoes         9345\n",
       "..                   ...                              ...          ...\n",
       "95                  Geox        Women Leather Solid Pumps        11990\n",
       "96                  Geox               Women Solid Wedges         8490\n",
       "97          Hush Puppies  Men Solid Leather Formal Derbys         9999\n",
       "98  Heel & Buckle London       Men Leather Formal Brogues         7693\n",
       "99               Saint G       Women Leather Heeled Boots         9900\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.myntra.com/shoes'\n",
    "#Fetch data from Myntra\n",
    "d, df = get_product_details_myntra(url, 100, True, 2, 'Black')\n",
    "d.quit()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” After setting the filters scrape first 10 laptops data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to scrape 3 attributes for each laptop:\n",
    "1. title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic used\n",
    "This is a dynamic website. The CPU TYPE filter facet sometimes allows us to select multiple options and sometimes it allows is to select only one option at a time. Hence I have written the following logic to overcome this:\n",
    "1. Select the 1st CPU Type value passes in the cpu_types list and fetch the data.\n",
    "2. click clear,and then use the next value sin th cpu_types list in the CPU Type filter facet and fetch the data.\n",
    "3. Repeat until the cpu_types list is exhausted.\n",
    "4. Finally, based on the number of products requested, display the data approximately equaly for all the cpu types used.\n",
    "\n",
    "Note: Since the prices and ratings information were not easy to fetch from the product list page using xpath, I have used regex to fetch the these information from the product list page itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highlevel_details_amazon(url, limit, cpu_types = ['all']):\n",
    "    \"\"\"\n",
    "    This function gets url, limit(numbr of rows to fetch), cpu_types(list of cpu types to use as filter) as input and\n",
    "    fetches the data in the form of a dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    titles = []\n",
    "    ratings = []\n",
    "    prices = []\n",
    "    cpu = []\n",
    "    \n",
    "    new_limit = limit\n",
    "    \n",
    "    #Regex patter to fetch ratings and prices\n",
    "    rating_pattern = r'\\d.\\d+ out of | \\d out of'\n",
    "    price_pattern = r'₹\\d+,\\d+,\\d+|₹\\d+,\\d+'\n",
    "    \n",
    "    #launching the website and searching with the search keyword\n",
    "    drivr = get_drive_launch(url)\n",
    "    drivr.find_element_by_id(\"twotabsearchtextbox\").send_keys(\"laptop\")\n",
    "    drivr.find_element_by_id(\"nav-search-submit-text\").click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    #Loopgin through the cpu type list to fetch data for the 'cpu type' input provided by the user\n",
    "    for cpu_type in cpu_types:\n",
    "        #Find and click the cpu type\n",
    "        try:\n",
    "            drivr.find_element_by_xpath(\"//li[@aria-label='\"+cpu_type+\"']//span\").click()\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #Run a while loop until the number of rows requested by the user is met\n",
    "        while len(titles) < new_limit:\n",
    "            #getting the webelements\n",
    "            product_elems = drivr.find_elements_by_xpath(\"//div[@class='a-section a-spacing-medium']\")\n",
    "            titles_elems = drivr.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']//span\")\n",
    "            \n",
    "            #Loop over the webelements to fetch the text from them\n",
    "            for title, prod_elem in zip(titles_elems, product_elems):\n",
    "                #Break if the number of items requested are met within for loop\n",
    "                if len(titles) == new_limit:\n",
    "                    break\n",
    "                titles.append(title.text)\n",
    "                cpu.append(cpu_type)\n",
    "                #Extracting the rating and price information based on the string pattern\n",
    "                rating = re.findall(rating_pattern, prod_elem.get_attribute('innerText'))\n",
    "                price = re.findall(price_pattern, prod_elem.get_attribute('innerText'))\n",
    "                \n",
    "                if len(rating) > 0:\n",
    "                    ratings.append(rating[0].split()[0])\n",
    "                else:\n",
    "                    ratings.append('No rating')\n",
    "                if len(price) > 0:\n",
    "                    prices.append(price[0])\n",
    "                else:\n",
    "                    prices.append('No price')\n",
    "            #Break if the number of items requested are met within while loop\n",
    "            if len(titles) == new_limit:\n",
    "                    break\n",
    "            #Go to the next page if more data is required\n",
    "            try:\n",
    "                url = drivr.find_element_by_xpath(\"//a[contains(text(),'Next')]\").get_attribute('href')\n",
    "            except NoSuchElementException:\n",
    "                break\n",
    "            drivr = get_drive_launch(url)\n",
    "            time.sleep(2)\n",
    "            \n",
    "        #Adjust the number of rows to pick so that we are fetching enough data for each cpu type provided by the user\n",
    "        new_limit = limit + new_limit\n",
    "        #Clearing the CPU TYPE filter in the website so that the new cpu type can be used\n",
    "        try:\n",
    "            drivr.find_element_by_xpath(\"//span[contains(text(),'Clear')]\").click()\n",
    "        except:\n",
    "            pass        \n",
    "        time.sleep(2)\n",
    "        \n",
    "    #Creating the initial dataframe with the whole data. \n",
    "    #The number of rows will be ('number of rows' requested by user) * (length of cpu_type list given by the user)\n",
    "    #mul\n",
    "    dataframe = pd.DataFrame({'Product Name':titles, 'Rating': ratings, 'Price': prices, 'CPU Type':cpu})\n",
    "        \n",
    "    #Here we are picking only the top few rows for each cpu type from the dataframe and create a new dataframe to\n",
    "    #match the number of rows requested by the user.\n",
    "    new_df = dataframe[0:(limit//len(cpu_types))]\n",
    "    start = limit\n",
    "    for i in range(1,len(cpu_types)):\n",
    "        end = (start + (limit//len(cpu_types)))\n",
    "        new_df = pd.concat([new_df, dataframe[start:end]], ignore_index=True)\n",
    "        start = start + limit\n",
    "        \n",
    "    #if the number of rows fall short, adjust it by filling it with the last few rows from the initial dataframe craeted\n",
    "    if new_df.shape[0] < limit:\n",
    "        new_df = pd.concat([new_df, dataframe[-(limit-new_df.shape[0]):]], ignore_index=True)\n",
    "    #return the driver and the new dataframe\n",
    "    return drivr, new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Price</th>\n",
       "      <th>CPU Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Renewed) HP ZBook 15 G3 Mobile Workstation - ...</td>\n",
       "      <td>No rating</td>\n",
       "      <td>₹83,990</td>\n",
       "      <td>Intel Core i7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Renewed) Lenovo Thinkpad Yoga S1 Laptop (CORE...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>₹38,990</td>\n",
       "      <td>Intel Core i7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HP 14 Thin &amp; Light 14-inch FHD Laptop (11th Ge...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>₹76,500</td>\n",
       "      <td>Intel Core i7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mi Notebook Horizon Edition 14 Intel Core i5-1...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>₹54,999</td>\n",
       "      <td>Intel Core i7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HP Pavilion Gaming 11th Gen Intel Core i7 Proc...</td>\n",
       "      <td>No rating</td>\n",
       "      <td>₹83,128</td>\n",
       "      <td>Intel Core i7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ASUS ZenBook Pro Duo Intel Core i9-10980HK 10t...</td>\n",
       "      <td>3.3</td>\n",
       "      <td>₹2,59,990</td>\n",
       "      <td>Intel Core i9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dell XPS 9570 15.6-inch UHD Laptop (8th Gen i9...</td>\n",
       "      <td>2.3</td>\n",
       "      <td>₹2,27,200</td>\n",
       "      <td>Intel Core i9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ASUS ROG Strix Scar 17 (2020), 17.3\" FHD 300Hz...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>₹2,78,490</td>\n",
       "      <td>Intel Core i9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ASUS ROG Strix Scar 15 (2020), 15.6\" FHD 300Hz...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>₹1,99,698</td>\n",
       "      <td>Intel Core i9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Apple MacBook Pro (16-inch, 16GB RAM, 1TB Stor...</td>\n",
       "      <td>3.7</td>\n",
       "      <td>₹2,24,900</td>\n",
       "      <td>Intel Core i9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name     Rating      Price  \\\n",
       "0  (Renewed) HP ZBook 15 G3 Mobile Workstation - ...  No rating    ₹83,990   \n",
       "1  (Renewed) Lenovo Thinkpad Yoga S1 Laptop (CORE...        1.0    ₹38,990   \n",
       "2  HP 14 Thin & Light 14-inch FHD Laptop (11th Ge...        4.6    ₹76,500   \n",
       "3  Mi Notebook Horizon Edition 14 Intel Core i5-1...        4.3    ₹54,999   \n",
       "4  HP Pavilion Gaming 11th Gen Intel Core i7 Proc...  No rating    ₹83,128   \n",
       "5  ASUS ZenBook Pro Duo Intel Core i9-10980HK 10t...        3.3  ₹2,59,990   \n",
       "6  Dell XPS 9570 15.6-inch UHD Laptop (8th Gen i9...        2.3  ₹2,27,200   \n",
       "7  ASUS ROG Strix Scar 17 (2020), 17.3\" FHD 300Hz...        4.8  ₹2,78,490   \n",
       "8  ASUS ROG Strix Scar 15 (2020), 15.6\" FHD 300Hz...        4.0  ₹1,99,698   \n",
       "9  Apple MacBook Pro (16-inch, 16GB RAM, 1TB Stor...        3.7  ₹2,24,900   \n",
       "\n",
       "        CPU Type  \n",
       "0  Intel Core i7  \n",
       "1  Intel Core i7  \n",
       "2  Intel Core i7  \n",
       "3  Intel Core i7  \n",
       "4  Intel Core i7  \n",
       "5  Intel Core i9  \n",
       "6  Intel Core i9  \n",
       "7  Intel Core i9  \n",
       "8  Intel Core i9  \n",
       "9  Intel Core i9  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Call the get_highlevel_details_amazon() function and pass the url and cpu type list as input\n",
    "url = 'https://www.amazon.in/'\n",
    "cpu_types = ['Intel Core i7','Intel Core i9']\n",
    "d, df = get_highlevel_details_amazon(url, 10, cpu_types)\n",
    "d.quit()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
