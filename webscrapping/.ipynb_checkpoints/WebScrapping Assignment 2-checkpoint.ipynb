{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_drive_launch(url):\n",
    "    driver = webdriver.Chrome('./driver/chromedriver.exe')\n",
    "    driver.maximize_window()\n",
    "    driver.implicitly_wait(10)\n",
    "    driver.get(url)\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highlevel_details_naukri(job, location = None, salary_range=None, stop=None, use_filter=False):\n",
    "    url = 'https://www.naukri.com/'\n",
    "    drivr = get_drive_launch(url)\n",
    "\n",
    "    drivr.find_element_by_id(\"qsb-keyword-sugg\").send_keys(job)\n",
    "    if ((location) and (not use_filter)):\n",
    "        drivr.find_element_by_id('qsb-location-sugg').send_keys(location)\n",
    "    time.sleep(2)\n",
    "    drivr.find_element_by_xpath(\"//div[@class='search-btn']/button\").click()\n",
    "    #click to expand location filter\n",
    "    \n",
    "    \n",
    "    if use_filter:        \n",
    "        if location:\n",
    "            time.sleep(3)\n",
    "            \n",
    "            try:\n",
    "                drivr.find_element_by_xpath(\"//span[@class = 'ellipsis fleft' and contains(text(), \"+\"'\"+location+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "            except:\n",
    "                drivr.find_element_by_xpath(\"//div[@data-filter-id='citiesGid']//a\").click()\n",
    "                time.sleep(3)\n",
    "                drivr.find_element_by_xpath(\"//span[@class = 'ellipsis fleft' and contains(text(), \"+\"'\"+location+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "        if salary_range:\n",
    "\n",
    "            try:\n",
    "                drivr.find_element_by_xpath(\"//span[contains(text(), \"+\"'\"+salary_range+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "            except:\n",
    "                drivr.find_element_by_xpath(\"//div[@data-filter-id='salaryRange']//a\").click()\n",
    "                time.sleep(3)\n",
    "                drivr.find_element_by_xpath(\"//span[contains(text(), \"+\"'\"+salary_range+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "\n",
    "    time.sleep(5)\n",
    "    print(drivr.find_element_by_xpath(\"//div[@class='sortAndH1Cont']/h1\").text)\n",
    "    titles=[]\n",
    "    companies=[]\n",
    "    exps=[]\n",
    "    salaries=[]\n",
    "    locs=[]\n",
    "\n",
    "    job_titles = drivr.find_elements_by_xpath(\"//div[@class='info fleft']/a\")\n",
    "    job_companies = drivr.find_elements_by_xpath(\"//div[@class='info fleft']/div/a[1]\")\n",
    "    job_exps = drivr.find_elements_by_xpath(\"//div[@class='info fleft']/ul/li[1]\")\n",
    "    job_salaries = drivr.find_elements_by_xpath(\"//div[@class='info fleft']/ul/li[2]\")\n",
    "    job_locations = drivr.find_elements_by_xpath(\"//div[@class='info fleft']/ul/li[3]\")\n",
    "    for job_title, job_company, job_exp, job_salary, job_loc in zip(job_titles[:stop],job_companies[:stop],job_exps[:stop],job_salaries[:stop],job_locations[:stop]):\n",
    "        titles.append(job_title.text)\n",
    "        companies.append(job_company.text)\n",
    "        exps.append(job_exp.text)\n",
    "        salaries.append(job_salary.text)\n",
    "        locs.append(job_loc.text)\n",
    "    \n",
    "    return drivr, pd.DataFrame({'Job Title':titles, 'Company':companies, 'Experience Required':exps,'Salary offered':salaries, 'Location':locs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_descrp_naukri(driver, stop = None):\n",
    "    main_window = driver.current_window_handle\n",
    "    all_windows = driver.window_handles\n",
    "\n",
    "    for window in all_windows:\n",
    "        if window != main_window:\n",
    "            driver.switch_to.window(window)\n",
    "            driver.close()\n",
    "\n",
    "    driver.switch_to.window(main_window)\n",
    "    main_window = driver.current_window_handle\n",
    "    all_windows = driver.window_handles\n",
    "\n",
    "    full_descriptions = []\n",
    "\n",
    "    for element in driver.find_elements_by_xpath(\"//div[@class='info fleft']/a\")[:stop]:\n",
    "        element.click()\n",
    "        time.sleep(2)\n",
    "        all_windows = driver.window_handles\n",
    "        driver.switch_to.window(all_windows[1])\n",
    "        try:\n",
    "            full_descriptions.append(driver.find_element_by_xpath(\"//section[@class='job-desc']\").text)\n",
    "        except:\n",
    "            full_descriptions.append(driver.find_element_by_xpath(\"//div[@class='clearboth description']\").text)\n",
    "        driver.close()\n",
    "        driver.switch_to.window(main_window)\n",
    "    return driver, full_descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_glassdoor(driver):\n",
    "    username = 'testacount573@gmail.com'\n",
    "    paswd = 'Testaccount123'\n",
    "    \n",
    "    main_window = driver.current_window_handle\n",
    "    try:\n",
    "        driver.find_element_by_class_name(\"locked-home-sign-in\").click()\n",
    "    except NoSuchElementException:\n",
    "        driver.find_element_by_class_xpath(\"//div[@class='d-flex']//button\").click()\n",
    "        \n",
    "    time.sleep(3)\n",
    "    #all_windows = driver.window_handles\n",
    "    #print(all_windows)\n",
    "    #driver.switch_to.window(all_windows[1])\n",
    "    \n",
    "    driver.find_element_by_id(\"userEmail\").send_keys(username)\n",
    "    driver.find_element_by_id(\"userPassword\").send_keys(paswd)\n",
    "    driver.find_element_by_xpath(\"//button[contains(text(),'Sign In')]\").click()\n",
    "    time.sleep(3)\n",
    "    driver.switch_to.window(main_window)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highlevel_details_glassdoor(job, location = None, salary_range=None, stop=None, use_filter=False):\n",
    "    url = 'https://www.glassdoor.co.in/index.htm'\n",
    "    drivr = get_drive_launch(url)\n",
    "    drivr = login_glassdoor(drivr)\n",
    "    drivr.find_element_by_id(\"sc.keyword\").send_keys(job)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    if ((location) and (not use_filter)):\n",
    "        drivr.find_element_by_xpath(\"//input[@data-test='search-bar-location-input']\").send_keys(Keys.CONTROL + \"a\")\n",
    "        drivr.find_element_by_xpath(\"//input[@data-test='search-bar-location-input']\").send_keys(Keys.DELETE)\n",
    "        drivr.find_element_by_xpath(\"//input[@data-test='search-bar-location-input']\").send_keys(location)\n",
    "    time.sleep(2)\n",
    "    drivr.find_element_by_xpath(\"//button[@type='submit']\").click()\n",
    "    #click to expand location filter \n",
    "    if use_filter:        \n",
    "        if location:\n",
    "            time.sleep(3)\n",
    "            \n",
    "            try:\n",
    "                drivr.find_element_by_xpath(\"//span[@class = 'ellipsis fleft' and contains(text(), \"+\"'\"+location+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "            except:\n",
    "                drivr.find_element_by_xpath(\"//div[@data-filter-id='citiesGid']//a\").click()\n",
    "                time.sleep(3)\n",
    "                drivr.find_element_by_xpath(\"//span[@class = 'ellipsis fleft' and contains(text(), \"+\"'\"+location+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "        if salary_range:\n",
    "\n",
    "            try:\n",
    "                drivr.find_element_by_xpath(\"//span[contains(text(), \"+\"'\"+salary_range+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "            except:\n",
    "                drivr.find_element_by_xpath(\"//div[@data-filter-id='salaryRange']//a\").click()\n",
    "                time.sleep(3)\n",
    "                drivr.find_element_by_xpath(\"//span[contains(text(), \"+\"'\"+salary_range+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "\n",
    "    time.sleep(5)\n",
    "    print(drivr.find_element_by_xpath(\"//h1[@data-test='jobTitle']\").text)\n",
    "    titles=[]\n",
    "    companies=[]\n",
    "    no_days_posts=[]\n",
    "    #salaries=[]\n",
    "    locs=[]\n",
    "    ratings=[]\n",
    "    \n",
    "    job_links = drivr.find_elements_by_xpath(\"//div[@class='d-flex flex-column css-x75kgh e1rrn5ka3']\")\n",
    "    \n",
    "    for job_link in job_links[:stop]:\n",
    "        try:\n",
    "            if job_link.text:\n",
    "                ratings.append(job_link.text)\n",
    "            else:\n",
    "                ratings.append('-')\n",
    "            #ratings.append(job_link.find_element_by_xpath(\"//span[@class='css-19pjha7 e1cjmv6j1']\").text)\n",
    "        except:\n",
    "            ratings.append(\"No Rating\")\n",
    "    job_titles = drivr.find_elements_by_xpath(\"//a[@data-test='job-link']/span\")\n",
    "    job_companies = drivr.find_elements_by_xpath(\"//div[@class='d-flex justify-content-between align-items-start']//a/span\")\n",
    "    job_days = drivr.find_elements_by_xpath(\"//div[@data-test='job-age']\")\n",
    "    #job_salaries = drivr.find_elements_by_xpath(\"//div[@class='info fleft']/ul/li[2]\")\n",
    "    job_locations = drivr.find_elements_by_xpath(\"//div[@class='d-flex flex-wrap css-11d3uq0 e1rrn5ka2']/span\")\n",
    "    \n",
    "    for job_title, job_company, job_day, job_loc in zip(job_titles[:stop],job_companies[:stop],job_days[:stop],job_locations[:stop]):\n",
    "        titles.append(job_title.text.strip())\n",
    "        companies.append(job_company.text.strip())\n",
    "        no_days_posts.append(job_day.text.strip())\n",
    "        #salaries.append(job_salary.text)\n",
    "        locs.append(job_loc.text.strip())\n",
    "    \n",
    "    return drivr, pd.DataFrame({'Job Title':titles, 'Company':companies, 'Job Post Age':no_days_posts, 'Location':locs, 'Ratings':ratings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_highlevel_details_flipkart(product_name, no_of_products = 100):\n",
    "    url = 'https://www.flipkart.com/'\n",
    "    driver = get_drive_launch(url)\n",
    "    driver.find_element_by_xpath(\"//button[contains(text(),'✕')]\").click()\n",
    "\n",
    "    driver.find_element_by_xpath(\"//input[@title='Search for products, brands and more']\").send_keys(product_name)\n",
    "    driver.find_element_by_xpath(\"//button[@type='submit']\").click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    product_names = []\n",
    "    product_descriptions = []\n",
    "    product_prices = []\n",
    "    product_discounts = []\n",
    "\n",
    "    while len(product_names) < no_of_products:\n",
    "        product_name_elems = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "        product_description_elems = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']/../a[not(@class='_3bPFwb')]\")\n",
    "        price_elems= driver.find_elements_by_xpath(\"//a[@class='_3bPFwb']//div[@class='_30jeq3']\")\n",
    "        price_discount_elems = driver.find_elements_by_xpath(\"//a[@class='_3bPFwb']\")\n",
    "        for prod_name_elem, product_description_elem, price_elem, price_discount_elem in zip(product_name_elems,product_description_elems,price_elems,price_discount_elems):\n",
    "            product_names.append(prod_name_elem.text.strip())\n",
    "            product_descriptions.append(product_description_elem.text.strip())\n",
    "            product_prices.append(price_elem.text.strip())\n",
    "            try:\n",
    "                discount = price_discount_elem.find_element_by_class_name(\"_3Ay6Sb\").text\n",
    "            except:\n",
    "                discount = None\n",
    "            if discount:\n",
    "                product_discounts.append(discount)\n",
    "            else:\n",
    "                product_discounts.append(\"0% off\")\n",
    "\n",
    "            if len(product_names) == no_of_products:\n",
    "                break\n",
    "        driver.find_element_by_xpath(\"//span[contains(text(),'Next')]\").click()\n",
    "        time.sleep(3)\n",
    "    dataframe = pd.DataFrame({'Product Brand': product_names, 'Product Description' : product_descriptions, 'Product Price' : product_prices,'Product Discount':product_discounts})\n",
    "    return driver, dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data. \n",
    "\n",
    "Note- All of the above steps have to be done in code. No step is to be done manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Analyst Jobs In Bangalore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience Required</th>\n",
       "      <th>Salary offered</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Assistant Manager II - Data Analyst</td>\n",
       "      <td>Flipkart Internet Private Limited</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Super India Tech Mark</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>1,25,000 - 2,25,000 PA.</td>\n",
       "      <td>Bangalore/Bengaluru(Devalapur)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>tech mahindra ltd</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>GlaxoSmithKline Pharmaceuticals Limited</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Xiaomi</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hiring For Data Analyst</td>\n",
       "      <td>Concentrix Daksh Services India Private Limited.</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>4,00,000 - 9,00,000 PA.</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Chennai, Bangalo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Cerner</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst with Marketing Analytics-Capco</td>\n",
       "      <td>Capco Technologies Pvt Ltd</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "      <td>7,00,000 - 17,00,000 PA.</td>\n",
       "      <td>Pune, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Business Data Analyst - MIS &amp; Reporting</td>\n",
       "      <td>INTERTRUST GROUP</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Mumbai, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Junior Data Analyst</td>\n",
       "      <td>Happy Marketer Private Ltd</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "      <td>3,00,000 - 7,00,000 PA.</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai (All Areas)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Job Title  \\\n",
       "0          Assistant Manager II - Data Analyst   \n",
       "1                                 Data Analyst   \n",
       "2                                 Data Analyst   \n",
       "3                                 Data Analyst   \n",
       "4                                 Data Analyst   \n",
       "5                      Hiring For Data Analyst   \n",
       "6                          Senior Data Analyst   \n",
       "7  Data Analyst with Marketing Analytics-Capco   \n",
       "8      Business Data Analyst - MIS & Reporting   \n",
       "9                          Junior Data Analyst   \n",
       "\n",
       "                                            Company Experience Required  \\\n",
       "0                 Flipkart Internet Private Limited             4-8 Yrs   \n",
       "1                             Super India Tech Mark             0-2 Yrs   \n",
       "2                                 tech mahindra ltd             4-8 Yrs   \n",
       "3           GlaxoSmithKline Pharmaceuticals Limited             2-7 Yrs   \n",
       "4                                            Xiaomi             2-6 Yrs   \n",
       "5  Concentrix Daksh Services India Private Limited.             2-7 Yrs   \n",
       "6                                            Cerner             3-5 Yrs   \n",
       "7                        Capco Technologies Pvt Ltd             4-9 Yrs   \n",
       "8                                  INTERTRUST GROUP             3-7 Yrs   \n",
       "9                        Happy Marketer Private Ltd             1-3 Yrs   \n",
       "\n",
       "             Salary offered                                           Location  \n",
       "0             Not disclosed                                Bangalore/Bengaluru  \n",
       "1   1,25,000 - 2,25,000 PA.                     Bangalore/Bengaluru(Devalapur)  \n",
       "2             Not disclosed                                Bangalore/Bengaluru  \n",
       "3             Not disclosed                                Bangalore/Bengaluru  \n",
       "4             Not disclosed                                Bangalore/Bengaluru  \n",
       "5   4,00,000 - 9,00,000 PA.  Hyderabad/Secunderabad, Pune, Chennai, Bangalo...  \n",
       "6             Not disclosed                                Bangalore/Bengaluru  \n",
       "7  7,00,000 - 17,00,000 PA.                          Pune, Bangalore/Bengaluru  \n",
       "8             Not disclosed                        Mumbai, Bangalore/Bengaluru  \n",
       "9   3,00,000 - 7,00,000 PA.            Bangalore/Bengaluru, Mumbai (All Areas)  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, dataFrame = get_highlevel_details_naukri('Data Analyst', location='Bangalore', stop=10)\n",
    "d.quit()\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note- \n",
    "1. All of the above steps have to be done in code. No step is to be done manually.\n",
    "2. Please note that you have to scrape full job description. For that you may have to open each job separately as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scientist Jobs In Bangalore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Full Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Required- Data Scientist (NLP)-Axis Bank - 6 m...</td>\n",
       "      <td>Axis Bank Limited</td>\n",
       "      <td>Kolkata, New Delhi, Hyderabad/Secunderabad, Pu...</td>\n",
       "      <td>Roles and Responsibilities - The bank generate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist - Python/ MATLAB/ Machine Learn...</td>\n",
       "      <td>Wrackle Technologies Pvt Ltd</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nData Scientist - Data Mining/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Principal Data Scientist - Machine/Deep Learni...</td>\n",
       "      <td>Fidius advisory</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nJob Description :\\n- We are l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist/ Analyst</td>\n",
       "      <td>Becton Dickinson India Pvt. Ltd</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nRoles and Responsibilities\\no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Opportunity For Data Scientist Internship - Be...</td>\n",
       "      <td>Corner Stone Solutions</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nLocation - Bangalore / Bengal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist || Data Analyst || Data science</td>\n",
       "      <td>Inspiration Manpower Consultancy Pvt. Ltd.</td>\n",
       "      <td>Navi Mumbai, Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nJob description\\nJob Summary ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Scientist, , Data Science</td>\n",
       "      <td>Visa Inc.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nWe are seeking an innovative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lead Data Scientist - Immediate Joiners are Re...</td>\n",
       "      <td>Techolution India Private Limited</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Pune, Chennai,...</td>\n",
       "      <td>Job description\\nWe are looking for qualified ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Scientist (Machine Vision solutions)</td>\n",
       "      <td>ONX Software Systems Pvt Ltd</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nSenior Data Scientist (Machin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Global Medical Data Scientist</td>\n",
       "      <td>GlaxoSmithKline Pharmaceuticals Limited</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>This is an ideal role for an experienced candi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0  Required- Data Scientist (NLP)-Axis Bank - 6 m...   \n",
       "1  Data Scientist - Python/ MATLAB/ Machine Learn...   \n",
       "2  Principal Data Scientist - Machine/Deep Learni...   \n",
       "3                            Data Scientist/ Analyst   \n",
       "4  Opportunity For Data Scientist Internship - Be...   \n",
       "5     Data Scientist || Data Analyst || Data science   \n",
       "6              Senior Data Scientist, , Data Science   \n",
       "7  Lead Data Scientist - Immediate Joiners are Re...   \n",
       "8   Senior Data Scientist (Machine Vision solutions)   \n",
       "9                      Global Medical Data Scientist   \n",
       "\n",
       "                                      Company  \\\n",
       "0                           Axis Bank Limited   \n",
       "1                Wrackle Technologies Pvt Ltd   \n",
       "2                             Fidius advisory   \n",
       "3             Becton Dickinson India Pvt. Ltd   \n",
       "4                      Corner Stone Solutions   \n",
       "5  Inspiration Manpower Consultancy Pvt. Ltd.   \n",
       "6                                   Visa Inc.   \n",
       "7           Techolution India Private Limited   \n",
       "8                ONX Software Systems Pvt Ltd   \n",
       "9     GlaxoSmithKline Pharmaceuticals Limited   \n",
       "\n",
       "                                            Location  \\\n",
       "0  Kolkata, New Delhi, Hyderabad/Secunderabad, Pu...   \n",
       "1                                Bangalore/Bengaluru   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5                   Navi Mumbai, Bangalore/Bengaluru   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7  Mumbai, Hyderabad/Secunderabad, Pune, Chennai,...   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                                    Full Description  \n",
       "0  Roles and Responsibilities - The bank generate...  \n",
       "1  Job description\\nData Scientist - Data Mining/...  \n",
       "2  Job description\\nJob Description :\\n- We are l...  \n",
       "3  Job description\\nRoles and Responsibilities\\no...  \n",
       "4  Job description\\nLocation - Bangalore / Bengal...  \n",
       "5  Job description\\nJob description\\nJob Summary ...  \n",
       "6  Job description\\nWe are seeking an innovative ...  \n",
       "7  Job description\\nWe are looking for qualified ...  \n",
       "8  Job description\\nSenior Data Scientist (Machin...  \n",
       "9  This is an ideal role for an experienced candi...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, dataFrame = get_highlevel_details_naukri('Data Scientist', location='Bangalore', stop=10)\n",
    "d, dataFrame['Full Description'] = get_full_descrp_naukri(d, 10)\n",
    "d.quit()\n",
    "dataFrame.drop(['Experience Required', 'Salary offered'], axis=1, inplace = True)\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Job description\\nJob Description :\\n- We are looking for a researcher who specializes in building personalization/recommender systems algorithm (ML APIs) for the applications mentioned above and work with our engineers to deploy them as scale.\\nWhat you will do :\\n- Apply your research expertise to build our ML-driven recommender system products, help us develop new solutions and unlock new directions, as well as analyse and optimise the systems we already. \\n- You'll work closely with product teams and mentor them on best practices for modern ML, and keep the wider team informed on the state-of-the-art. In addition, you will be in a strategic position to influence future roadmaps for recommender system products.\\n- Collaborate with a cross-functional agile team spanning user research, design, data science, product management, and engineering to build new product features that advance our mission to connect artists and fans in personalized and relevant ways.\\n- Prototype new approaches and production-ize solutions at scale for our hundreds of thousands of active users. Help drive optimization, testing, and tools to improve quality.\\nRequirements :\\n- Master, Post-graduate or Ph.D. in computer science, machine learning, information retrieval, recommendation systems, natural language processing, statistics, math, engineering, operations research, or another quantitative discipline; or equivalent work experience.\\n- Good theoretical grounding in core machine learning concepts and techniques.\\n- Ability to perform comprehensive literature reviews and provide critical feedback on state-of-the-art solutions and how they may fit different operating constraints.\\n- Experience with a number of ML techniques and frameworks, e.g. Natural Language Processing, Recommender Systems, sampling, linear regression, decision trees, SVMs, deep neural networks, etc.\\n- Familiarity with one or more Deep learning software frameworks such as Tensorflow, PyTorch.\\nRoleAnalytics Manager\\nIndustry TypeIT-Software, Software Services\\nFunctional AreaAnalytics & Business Intelligence\\nEmployment TypeFull Time, Permanent\\nRole CategoryAnalytics & BI\\nEducation\\nUG :Any Graduate in Any Specialization\\nPG :M.Tech in Computers, MS/M.Sc(Science) in Computers\\nKey Skills\\nTensorflowNLPPyTorchdata scienceproduct managementdesignDeep Learning\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame.loc[2, 'Full Description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You have to use the location and salary filter.\n",
    "2. You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "3. You have to scrape the job-title, job-location, company_name, experience_required.\n",
    "\n",
    "1. The location filter to be used is “Delhi/NCR”\n",
    "2. The salary filter to be used is “3-6” lakhs\n",
    "\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field .WEB SCRAPING ASSIGNMENT-2.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note- All of the above steps have to be done in code. No step is to be done \n",
    "manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scientist Jobs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience Required</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Developer - Data Science</td>\n",
       "      <td>ICL Systems India Private Limited</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "      <td>Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Required- Data Scientist (NLP)-Axis Bank - 6 m...</td>\n",
       "      <td>Axis Bank Limited</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "      <td>Kolkata, New Delhi, Hyderabad/Secunderabad, Pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist/Data Analyst - Python/Machine L...</td>\n",
       "      <td>Change leaders</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Mumbai, Ghaziabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business Consultant Data Scientist</td>\n",
       "      <td>StriveX Consulting Pvt. Ltd</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "      <td>Hyderabad/Secunderabad, Chennai, Bangalore/Ben...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Amity University</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "      <td>Ghaziabad, Faridabad, Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist - Python &amp; Machine Learning</td>\n",
       "      <td>FUTURES AND CAREERS</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Bangalore/Bengal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist - Python &amp; Machine Learning</td>\n",
       "      <td>FUTURES AND CAREERS</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Hyderabad/Secunderabad, Bangalore/Bengaluru, D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist - Machine Learning/ NLP</td>\n",
       "      <td>TalPro</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hiring For Data Scientist</td>\n",
       "      <td>Max Bupa Health Insurance Company Limited</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram, Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist - Python / Machine Learning / T...</td>\n",
       "      <td>FUTURES AND CAREERS</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Hyderabad/Secunderabad, Bangalore/Bengaluru, D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0                           Developer - Data Science   \n",
       "1  Required- Data Scientist (NLP)-Axis Bank - 6 m...   \n",
       "2  Data Scientist/Data Analyst - Python/Machine L...   \n",
       "3                 Business Consultant Data Scientist   \n",
       "4                                     Data Scientist   \n",
       "5         Data Scientist - Python & Machine Learning   \n",
       "6         Data Scientist - Python & Machine Learning   \n",
       "7             Data Scientist - Machine Learning/ NLP   \n",
       "8                          Hiring For Data Scientist   \n",
       "9  Data Scientist - Python / Machine Learning / T...   \n",
       "\n",
       "                                     Company Experience Required  \\\n",
       "0          ICL Systems India Private Limited             3-5 Yrs   \n",
       "1                          Axis Bank Limited             4-9 Yrs   \n",
       "2                             Change leaders            5-10 Yrs   \n",
       "3                StriveX Consulting Pvt. Ltd             2-4 Yrs   \n",
       "4                           Amity University             6-8 Yrs   \n",
       "5                        FUTURES AND CAREERS             2-7 Yrs   \n",
       "6                        FUTURES AND CAREERS             2-7 Yrs   \n",
       "7                                     TalPro             2-6 Yrs   \n",
       "8  Max Bupa Health Insurance Company Limited             1-6 Yrs   \n",
       "9                        FUTURES AND CAREERS             3-8 Yrs   \n",
       "\n",
       "                                            Location  \n",
       "0                                        Delhi / NCR  \n",
       "1  Kolkata, New Delhi, Hyderabad/Secunderabad, Pu...  \n",
       "2                                  Mumbai, Ghaziabad  \n",
       "3  Hyderabad/Secunderabad, Chennai, Bangalore/Ben...  \n",
       "4                  Ghaziabad, Faridabad, Delhi / NCR  \n",
       "5  Hyderabad/Secunderabad, Pune, Bangalore/Bengal...  \n",
       "6  Hyderabad/Secunderabad, Bangalore/Bengaluru, D...  \n",
       "7                                   Gurgaon/Gurugram  \n",
       "8                      Gurgaon/Gurugram, Delhi / NCR  \n",
       "9  Hyderabad/Secunderabad, Bangalore/Bengaluru, D...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver,dataFrame=get_highlevel_details_naukri('Data Scientist', location = 'Delhi / NCR', salary_range='3-6 Lakhs', stop=10, use_filter=True)\n",
    "driver.quit()\n",
    "dataFrame.drop(['Salary offered'],axis=1,inplace=True)\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "2. Enter “Data Scientist” in “Job Title,Keyword,Company” field and enter “Noida” in “location” field.\n",
    "3. Then click the search button. You will land up in the below page:\n",
    "4. Then scrape the data for the first 10 jobs results you get in the above shown page.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note- All of the above steps have to be done in code. No step is to be done\n",
    "manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scientist Jobs in Noida\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Job Post Age</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Innovaccer Analytics Private Limited</td>\n",
       "      <td>8d</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Asquero</td>\n",
       "      <td>24h</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unyscape Infocom Pvt. Ltd</td>\n",
       "      <td>25d</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Biz2Credit Inc</td>\n",
       "      <td>30d+</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Techlive</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Salasar New Age Technologies</td>\n",
       "      <td>30d+</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Adobe</td>\n",
       "      <td>5d</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SysQuo Innovation Private Limited</td>\n",
       "      <td>9d</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Salasar New Age Technologies</td>\n",
       "      <td>30d+</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IBM</td>\n",
       "      <td>5d</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Company Job Post Age Ratings\n",
       "0  Innovaccer Analytics Private Limited           8d     3.5\n",
       "1                               Asquero          24h     5.0\n",
       "2             Unyscape Infocom Pvt. Ltd          25d     4.1\n",
       "3                        Biz2Credit Inc         30d+     3.8\n",
       "4                              Techlive                    -\n",
       "5          Salasar New Age Technologies         30d+       -\n",
       "6                                 Adobe           5d     4.4\n",
       "7     SysQuo Innovation Private Limited           9d     3.0\n",
       "8          Salasar New Age Technologies         30d+       -\n",
       "9                                   IBM           5d     4.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver, dataframe = get_highlevel_details_glassdoor('Data Scientist','Noida',stop=10)\n",
    "driver.quit()\n",
    "dataframe=dataframe[['Company', 'Job Post Age', 'Ratings']]\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location. You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above task will be, done as shown in the below steps:\n",
    "1. first get the webpage https://www.glassdoor.co.in/Salaries/index.htm\n",
    "2. Enter “Data Scientist” in Job title field and “Noida” in location field.\n",
    "3. Click the search button.\n",
    "4. After that you will land on the below page You have to scrape whole data from this webpage\n",
    "5. Scrape data for first 10 companies. Scrape the min salary, max salary, company name, Average salary and rating of the company.\n",
    "6. Store the data in a dataframe.\n",
    "\n",
    "Note that all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'two', 't', 'h', 'r', 'e', 'e']\n"
     ]
    }
   ],
   "source": [
    "x = ['one','two']\n",
    "x.extend('three')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_glassdoor_salary(driver):\n",
    "    username = 'testacount573@gmail.com'\n",
    "    paswd = 'Testaccount123'\n",
    "    \n",
    "    main_window = driver.current_window_handle\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//li[@class='sign-in']/a\").click()\n",
    "    except NoSuchElementException:\n",
    "        #driver.find_element_by_class_xpath(\"//div[@class='d-flex']//button\").click()\n",
    "        pass\n",
    "        \n",
    "    time.sleep(3)\n",
    "    #all_windows = driver.window_handles\n",
    "    #print(all_windows)\n",
    "    #driver.switch_to.window(all_windows[1])\n",
    "    \n",
    "    driver.find_element_by_id(\"userEmail\").send_keys(username)\n",
    "    driver.find_element_by_id(\"userPassword\").send_keys(paswd)\n",
    "    driver.find_element_by_xpath(\"//button[contains(text(),'Sign In')]\").click()\n",
    "    time.sleep(3)\n",
    "    driver.switch_to.window(main_window)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_salary_details_glassdoor(job_title, location, stop=None):\n",
    "    url = 'https://www.glassdoor.co.in/Salaries/index.htm'\n",
    "    drivr = get_drive_launch(url)\n",
    "    drivr = login_glassdoor_salary(drivr)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    drivr.find_element_by_id(\"KeywordSearch\").send_keys(job_title)\n",
    "    drivr.find_element_by_id(\"LocationSearch\").send_keys(Keys.CONTROL + \"a\")\n",
    "    drivr.find_element_by_id(\"LocationSearch\").send_keys(Keys.DELETE)\n",
    "    drivr.find_element_by_id(\"LocationSearch\").send_keys(location)\n",
    "    drivr.find_element_by_id(\"HeroSearchButton\").click()\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6 : Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount %\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape the data you have to go through following steps:\n",
    "1. Go to flipkart webpage by url https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon\n",
    "3. after that you will reach to a webpage having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "4. after scraping data from the first page, go to the “Next” Button at the bottom of the page , then click on it\n",
    "5. Now scrape data from this page as usual\n",
    "6. repeat this until you get data for 100 sunglasses.\n",
    "\n",
    "Note that all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Product Price</th>\n",
       "      <th>Product Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Round Sunglasses (55)</td>\n",
       "      <td>₹449</td>\n",
       "      <td>82% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Butterfly Sunglasses (55)</td>\n",
       "      <td>₹209</td>\n",
       "      <td>86% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹758</td>\n",
       "      <td>15% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹569</td>\n",
       "      <td>28% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Mirrored, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ALEYBEE</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹189</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>GANSTA</td>\n",
       "      <td>UV Protection, Night Vision, Riding Glasses Av...</td>\n",
       "      <td>₹295</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Round Sunglasses (Free...</td>\n",
       "      <td>₹399</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>hipe</td>\n",
       "      <td>UV Protection Round Sunglasses (Free Size)</td>\n",
       "      <td>₹228</td>\n",
       "      <td>82% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NuVew</td>\n",
       "      <td>UV Protection, Mirrored Clubmaster Sunglasses ...</td>\n",
       "      <td>₹167</td>\n",
       "      <td>77% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Product Brand                                Product Description  \\\n",
       "0           PIRASO                UV Protection Round Sunglasses (55)   \n",
       "1           PIRASO            UV Protection Butterfly Sunglasses (55)   \n",
       "2         Fastrack      UV Protection Wayfarer Sunglasses (Free Size)   \n",
       "3         Fastrack   UV Protection Rectangular Sunglasses (Free Size)   \n",
       "4         Fastrack  Mirrored, UV Protection Wayfarer Sunglasses (F...   \n",
       "..             ...                                                ...   \n",
       "95         ALEYBEE                UV Protection Round Sunglasses (54)   \n",
       "96          GANSTA  UV Protection, Night Vision, Riding Glasses Av...   \n",
       "97  ROZZETTA CRAFT  UV Protection, Gradient Round Sunglasses (Free...   \n",
       "98            hipe         UV Protection Round Sunglasses (Free Size)   \n",
       "99           NuVew  UV Protection, Mirrored Clubmaster Sunglasses ...   \n",
       "\n",
       "   Product Price Product Discount  \n",
       "0           ₹449          82% off  \n",
       "1           ₹209          86% off  \n",
       "2           ₹758          15% off  \n",
       "3           ₹569          28% off  \n",
       "4           ₹499          50% off  \n",
       "..           ...              ...  \n",
       "95          ₹189          87% off  \n",
       "96          ₹295          85% off  \n",
       "97          ₹399          80% off  \n",
       "98          ₹228          82% off  \n",
       "99          ₹167          77% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, dataFrame = get_product_highlevel_details_flipkart('sunglasses', 100)\n",
    "d.quit()\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes\u0002earpods-power\u0002adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you will open the above link you will reach to the below shown webpage.\n",
    "\n",
    "As shown in the above page you have to scrape the tick marked attributes.\n",
    "These are \n",
    "1. Rating \n",
    "2. Review_summary \n",
    "3. Full review\n",
    "\n",
    "You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews_flipkart(product_url, no_of_reviews):\n",
    "    drivr = get_drive_launch(product_url)\n",
    "    product_reviews_url = drivr.find_element_by_xpath(\"//div[@class='_3UAT2v _16PBlm']/..\").get_attribute('href')\n",
    "    drivr.quit()\n",
    "    drivr = get_drive_launch(product_reviews_url)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    ratings = []\n",
    "    review_summaries = []\n",
    "    full_reviews = []\n",
    "\n",
    "    while len(ratings) < no_of_reviews:\n",
    "        rating_elems = drivr.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "        review_summary_elems = drivr.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "        full_review_elems= drivr.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "\n",
    "        for rating_elem,review_summary_elem,full_review_elem in zip(rating_elems,review_summary_elems,full_review_elems):\n",
    "            ratings.append(rating_elem.text.strip())\n",
    "            review_summaries.append(review_summary_elem.text.strip())\n",
    "            full_reviews.append(full_review_elem.text.strip())\n",
    "            if len(ratings) == no_of_reviews:\n",
    "                break\n",
    "        drivr.find_element_by_xpath(\"//span[contains(text(),'Next')]\").click()\n",
    "        time.sleep(3)\n",
    "    dataframe = pd.DataFrame({'Rating': ratings, 'Review Summary' : review_summaries, 'Full Review' : full_reviews})\n",
    "    return drivr, dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Summary</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the Money\\n\\nThe iPhone 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.\\n\\nI’m am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>iphone 11 is a very good phone to buy only if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>4</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>impressive super phone and best in class camer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Best and amazing product.....phone looks so pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>Everything is perfect pictures come out so cle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific</td>\n",
       "      <td>I got this beast today. And I must say the pic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>iPhone is delivered on time. Display is great ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating      Review Summary  \\\n",
       "0       5           Brilliant   \n",
       "1       5    Perfect product!   \n",
       "2       5       Great product   \n",
       "3       5   Worth every penny   \n",
       "4       5  Highly recommended   \n",
       "..    ...                 ...   \n",
       "95      4           Very Good   \n",
       "96      5    Perfect product!   \n",
       "97      5           Fabulous!   \n",
       "98      5            Terrific   \n",
       "99      5   Worth every penny   \n",
       "\n",
       "                                          Full Review  \n",
       "0   The Best Phone for the Money\\n\\nThe iPhone 11 ...  \n",
       "1   Amazing phone with great cameras and better ba...  \n",
       "2   Amazing Powerful and Durable Gadget.\\n\\nI’m am...  \n",
       "3   Previously I was using one plus 3t it was a gr...  \n",
       "4   iphone 11 is a very good phone to buy only if ...  \n",
       "..                                                ...  \n",
       "95  impressive super phone and best in class camer...  \n",
       "96  Best and amazing product.....phone looks so pr...  \n",
       "97  Everything is perfect pictures come out so cle...  \n",
       "98  I got this beast today. And I must say the pic...  \n",
       "99  iPhone is delivered on time. Display is great ...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_url = 'https://www.flipkart.com/apple-iphone-11-black-64-gb-includes%02earpods-power%02adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace'\n",
    "d, DataFrame = get_reviews_flipkart(product_url, 100)\n",
    "d.quit()\n",
    "DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to scrape 4 attributes of each sneaker :\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. discount %\n",
    "\n",
    "As shown in the below image, you have to scrape the tick marked attributes\n",
    "\n",
    "Also note that all the steps required during scraping should be done through code \n",
    "only and not manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Product Price</th>\n",
       "      <th>Product Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K' Footlance</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹499</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Combo Pack Of 4 Casual Sneakers For Men</td>\n",
       "      <td>₹449</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Perfect &amp; Affordable Combo Pack of 02 Pairs Sn...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>72% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Combo Pack of 4 Casual Sneakers With Sneakers ...</td>\n",
       "      <td>₹474</td>\n",
       "      <td>76% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Robbie jones</td>\n",
       "      <td>Casual Sneakers Shoes For Men Sneakers For Men</td>\n",
       "      <td>₹360</td>\n",
       "      <td>63% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>SHOEFLY</td>\n",
       "      <td>Combo Men Pack of 2 Loafers Shoes Sneakers For...</td>\n",
       "      <td>₹343</td>\n",
       "      <td>65% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Smart Casuals Canvas Shoes Combo pack of 2 Sne...</td>\n",
       "      <td>₹399</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>BERSACHE</td>\n",
       "      <td>Combo pack of 2 casual, sneaker and loafer sho...</td>\n",
       "      <td>₹498</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>World Wear Footwear</td>\n",
       "      <td>Latest Collection-1227 Stylish Casual Sports S...</td>\n",
       "      <td>₹259</td>\n",
       "      <td>48% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NBA</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹699</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Product Brand                                Product Description  \\\n",
       "0          K' Footlance                                   Sneakers For Men   \n",
       "1                BRUTON            Combo Pack Of 4 Casual Sneakers For Men   \n",
       "2                Chevit  Perfect & Affordable Combo Pack of 02 Pairs Sn...   \n",
       "3                Chevit  Combo Pack of 4 Casual Sneakers With Sneakers ...   \n",
       "4          Robbie jones     Casual Sneakers Shoes For Men Sneakers For Men   \n",
       "..                  ...                                                ...   \n",
       "95              SHOEFLY  Combo Men Pack of 2 Loafers Shoes Sneakers For...   \n",
       "96               Chevit  Smart Casuals Canvas Shoes Combo pack of 2 Sne...   \n",
       "97             BERSACHE  Combo pack of 2 casual, sneaker and loafer sho...   \n",
       "98  World Wear Footwear  Latest Collection-1227 Stylish Casual Sports S...   \n",
       "99                  NBA                                   Sneakers For Men   \n",
       "\n",
       "   Product Price Product Discount  \n",
       "0           ₹499          50% off  \n",
       "1           ₹449          88% off  \n",
       "2           ₹499          72% off  \n",
       "3           ₹474          76% off  \n",
       "4           ₹360          63% off  \n",
       "..           ...              ...  \n",
       "95          ₹343          65% off  \n",
       "96          ₹399          60% off  \n",
       "97          ₹498          50% off  \n",
       "98          ₹259          48% off  \n",
       "99          ₹699          80% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, dataFrame = get_product_highlevel_details_flipkart('sneakers', 100)\n",
    "d.quit()\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9: Go to the link - https://www.myntra.com/shoes Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black” and then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe Please note that applying the filter and scraping the data , everything should be done through code only and there should not be any manual step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_details_myntra(url, num_of_items = 10, use_filter = False, price_range_item_num = 1, color = 'White'):\n",
    "    drivr = get_drive_launch(url)\n",
    "    if use_filter:\n",
    "        drivr.find_element_by_xpath(\"//ul[@class='price-list']/li[\"+str(price_range_item_num)+\"]\").click()\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            drivr.find_element_by_xpath(\"//li[@class='colour-listItem']/label[(contains(text(),'\"+color+\"'))]\").click()\n",
    "        except:\n",
    "            drivr.find_element_by_class_name(\"colour-more\").click()\n",
    "            drivr.find_element_by_xpath(\"//li[@class='colour-listItem']/label[(contains(text(),'\"+color+\"'))]\").click()\n",
    "        time.sleep(2)\n",
    "\n",
    "    product_names = []\n",
    "    product_short_desc = []\n",
    "    prices = []\n",
    "\n",
    "    while (len(product_names) < num_of_items):        \n",
    "        brand_name_elems = drivr.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "        short_desc_elems = drivr.find_elements_by_xpath(\"//h4[@class='product-product']\")        \n",
    "        price_elems = drivr.find_elements_by_xpath(\"//div[@class='product-price']\")\n",
    "        for brand_name, short_desc, price in zip(brand_name_elems, short_desc_elems, price_elems):\n",
    "            product_names.append(brand_name.text)\n",
    "            product_short_desc.append(short_desc.text)\n",
    "            prices.append(price.text.split('Rs. ')[1])\n",
    "            if len(product_names) == num_of_items:\n",
    "                break\n",
    "        if len(product_names) == num_of_items:\n",
    "                break\n",
    "        url = drivr.find_element_by_xpath(\"//a[@rel='next']\").get_attribute('href')\n",
    "        drivr.quit()\n",
    "        drivr = get_drive_launch(url)\n",
    "        time.sleep(3)\n",
    "    datafrm = pd.DataFrame({'Product Name':product_names, 'Product Short Description': product_short_desc, 'Price in INR':prices})\n",
    "    return drivr, datafrm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Product Short Description</th>\n",
       "      <th>Price in INR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Men Fuse Training Sports Shoes</td>\n",
       "      <td>7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nike</td>\n",
       "      <td>AIR ZOOM PEGASUS Running Shoes</td>\n",
       "      <td>11495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men AIR ZOOM Running Shoes</td>\n",
       "      <td>11470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PUMA Motorsport</td>\n",
       "      <td>Unisex Mercedes Running Shoes</td>\n",
       "      <td>7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men REACT MILER Running Shoes</td>\n",
       "      <td>9345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>VIONIC</td>\n",
       "      <td>Men Textured Sneakers</td>\n",
       "      <td>7039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Hush Puppies</td>\n",
       "      <td>Men Colourblocked Driving Shoes</td>\n",
       "      <td>8999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Men IGNITE Dual Running Shoes</td>\n",
       "      <td>6999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Men IGNITE Ronin Unrest</td>\n",
       "      <td>6999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Women Leather Pumps</td>\n",
       "      <td>9999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Product Name        Product Short Description Price in INR\n",
       "0              Puma   Men Fuse Training Sports Shoes         7999\n",
       "1              Nike   AIR ZOOM PEGASUS Running Shoes        11495\n",
       "2              Nike       Men AIR ZOOM Running Shoes        11470\n",
       "3   PUMA Motorsport    Unisex Mercedes Running Shoes         7999\n",
       "4              Nike    Men REACT MILER Running Shoes         9345\n",
       "..              ...                              ...          ...\n",
       "95           VIONIC            Men Textured Sneakers         7039\n",
       "96     Hush Puppies  Men Colourblocked Driving Shoes         8999\n",
       "97             Puma    Men IGNITE Dual Running Shoes         6999\n",
       "98             Puma          Men IGNITE Ronin Unrest         6999\n",
       "99             Geox              Women Leather Pumps         9999\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.myntra.com/shoes'\n",
    "d, df = get_product_details_myntra(url, 100, True, 2, 'Black')\n",
    "d.quit()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_details_myntra(url, num_of_items = 10, use_filter = False, price_range_item_num = 1, color = 'White'):\n",
    "    drivr = get_drive_launch(url)\n",
    "    if use_filter:\n",
    "        drivr.find_element_by_xpath(\"//ul[@class='price-list']/li[\"+str(price_range_item_num)+\"]\").click()\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            drivr.find_element_by_xpath(\"//li[@class='colour-listItem']/label[(contains(text(),'\"+color+\"'))]\").click()\n",
    "        except:\n",
    "            drivr.find_element_by_class_name(\"colour-more\").click()\n",
    "            drivr.find_element_by_xpath(\"//li[@class='colour-listItem']/label[(contains(text(),'\"+color+\"'))]\").click()\n",
    "        time.sleep(2)\n",
    "\n",
    "    product_names = []\n",
    "    product_short_desc = []\n",
    "    prices = []\n",
    "\n",
    "    while (len(product_names) < num_of_items):        \n",
    "        brand_name_elems = drivr.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "        short_desc_elems = drivr.find_elements_by_xpath(\"//h4[@class='product-product']\")        \n",
    "        price_elems = drivr.find_elements_by_xpath(\"//div[@class='product-price']\")\n",
    "        for brand_name, short_desc, price in zip(brand_name_elems, short_desc_elems, price_elems):\n",
    "            product_names.append(brand_name.text)\n",
    "            product_short_desc.append(short_desc.text)\n",
    "            prices.append(price.text.split('Rs. ')[1])\n",
    "            if len(product_names) == num_of_items:\n",
    "                break\n",
    "        if len(product_names) == num_of_items:\n",
    "                break\n",
    "        url = drivr.find_element_by_xpath(\"//a[@rel='next']\").get_attribute('href')\n",
    "        drivr.quit()\n",
    "        drivr = get_drive_launch(url)\n",
    "        time.sleep(3)\n",
    "    datafrm = pd.DataFrame({'Product Name':product_names, 'Product Short Description': product_short_desc, 'Price in INR':prices})\n",
    "    return drivr, datafrm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” After setting the filters scrape first 10 laptops data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to scrape 3 attributes for each laptop:\n",
    "1. title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//span[contains(text(),'Intel Core i9')]\"}\n  (Session info: chrome=89.0.4389.90)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-4e3e74daafdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdrivr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"//span[contains(text(),'Intel Core i7')]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdrivr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"//span[contains(text(),'Intel Core i9')]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mX:\\AnacondaEnvironment\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element_by_xpath\u001b[1;34m(self, xpath)\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[0melement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'//div/td[1]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \"\"\"\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_elements_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mX:\\AnacondaEnvironment\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    976\u001b[0m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[0;32m    977\u001b[0m             \u001b[1;34m'using'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m             'value': value})['value']\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mX:\\AnacondaEnvironment\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32mX:\\AnacondaEnvironment\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//span[contains(text(),'Intel Core i9')]\"}\n  (Session info: chrome=89.0.4389.90)\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.amazon.in/'\n",
    "drivr = get_drive_launch(url)\n",
    "drivr.find_element_by_id(\"twotabsearchtextbox\").send_keys(\"laptop\")\n",
    "drivr.find_element_by_id(\"nav-search-submit-text\").click()\n",
    "time.sleep(2)\n",
    "drivr.find_element_by_xpath(\"//span[contains(text(),'Intel Core i7')]\").click()\n",
    "time.sleep(2)\n",
    "drivr.find_element_by_xpath(\"//span[contains(text(),'Intel Core i9')]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if use_filter:\n",
    "    drivr.find_element_by_xpath(\"//ul[@class='price-list']/li[\"+str(price_range_item_num)+\"]\").click()\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        drivr.find_element_by_xpath(\"//li[@class='colour-listItem']/label[(contains(text(),'\"+color+\"'))]\").click()\n",
    "    except:\n",
    "        drivr.find_element_by_class_name(\"colour-more\").click()\n",
    "        drivr.find_element_by_xpath(\"//li[@class='colour-listItem']/label[(contains(text(),'\"+color+\"'))]\").click()\n",
    "    time.sleep(2)\n",
    "\n",
    "product_names = []\n",
    "product_short_desc = []\n",
    "prices = []\n",
    "\n",
    "while (len(product_names) < num_of_items):        \n",
    "    brand_name_elems = drivr.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "    short_desc_elems = drivr.find_elements_by_xpath(\"//h4[@class='product-product']\")        \n",
    "    price_elems = drivr.find_elements_by_xpath(\"//div[@class='product-price']\")\n",
    "    for brand_name, short_desc, price in zip(brand_name_elems, short_desc_elems, price_elems):\n",
    "        product_names.append(brand_name.text)\n",
    "        product_short_desc.append(short_desc.text)\n",
    "        prices.append(price.text.split('Rs. ')[1])\n",
    "        if len(product_names) == num_of_items:\n",
    "            break\n",
    "    if len(product_names) == num_of_items:\n",
    "            break\n",
    "    url = drivr.find_element_by_xpath(\"//a[@rel='next']\").get_attribute('href')\n",
    "    drivr.quit()\n",
    "    drivr = get_drive_launch(url)\n",
    "    time.sleep(3)\n",
    "pd.DataFrame({'Product Name':product_names, 'Product Short Description': product_short_desc, 'Price in INR':prices})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
