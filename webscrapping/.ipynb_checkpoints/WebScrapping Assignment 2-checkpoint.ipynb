{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_drive_launch(url):\n",
    "    driver = webdriver.Chrome('./driver/chromedriver.exe')\n",
    "    driver.maximize_window()\n",
    "    driver.implicitly_wait(10)\n",
    "    driver.get(url)\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highlevel_details_naukri(job, location = None, salary_range=None, stop=None, use_filter=False):\n",
    "    url = 'https://www.naukri.com/'\n",
    "    drivr = get_drive_launch(url)\n",
    "\n",
    "    drivr.find_element_by_id(\"qsb-keyword-sugg\").send_keys(job)\n",
    "    if ((location) and (not use_filter)):\n",
    "        drivr.find_element_by_id('qsb-location-sugg').send_keys(location)\n",
    "    time.sleep(2)\n",
    "    drivr.find_element_by_xpath(\"//div[@class='search-btn']/button\").click()\n",
    "    #click to expand location filter\n",
    "    \n",
    "    \n",
    "    if use_filter:        \n",
    "        if location:\n",
    "            time.sleep(3)\n",
    "            \n",
    "            try:\n",
    "                drivr.find_element_by_xpath(\"//span[@class = 'ellipsis fleft' and contains(text(), \"+\"'\"+location+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "            except:\n",
    "                drivr.find_element_by_xpath(\"//div[@data-filter-id='citiesGid']//a\").click()\n",
    "                time.sleep(3)\n",
    "                drivr.find_element_by_xpath(\"//span[@class = 'ellipsis fleft' and contains(text(), \"+\"'\"+location+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "        if salary_range:\n",
    "\n",
    "            try:\n",
    "                drivr.find_element_by_xpath(\"//span[contains(text(), \"+\"'\"+salary_range+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "            except:\n",
    "                drivr.find_element_by_xpath(\"//div[@data-filter-id='salaryRange']//a\").click()\n",
    "                time.sleep(3)\n",
    "                drivr.find_element_by_xpath(\"//span[contains(text(), \"+\"'\"+salary_range+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "\n",
    "    time.sleep(5)\n",
    "    print(drivr.find_element_by_xpath(\"//div[@class='sortAndH1Cont']/h1\").text)\n",
    "    titles=[]\n",
    "    companies=[]\n",
    "    exps=[]\n",
    "    salaries=[]\n",
    "    locs=[]\n",
    "\n",
    "    job_titles = drivr.find_elements_by_xpath(\"//div[@class='info fleft']/a\")\n",
    "    job_companies = drivr.find_elements_by_xpath(\"//div[@class='info fleft']/div/a[1]\")\n",
    "    job_exps = drivr.find_elements_by_xpath(\"//div[@class='info fleft']/ul/li[1]\")\n",
    "    job_salaries = drivr.find_elements_by_xpath(\"//div[@class='info fleft']/ul/li[2]\")\n",
    "    job_locations = drivr.find_elements_by_xpath(\"//div[@class='info fleft']/ul/li[3]\")\n",
    "    for job_title, job_company, job_exp, job_salary, job_loc in zip(job_titles[:stop],job_companies[:stop],job_exps[:stop],job_salaries[:stop],job_locations[:stop]):\n",
    "        titles.append(job_title.text)\n",
    "        companies.append(job_company.text)\n",
    "        exps.append(job_exp.text)\n",
    "        salaries.append(job_salary.text)\n",
    "        locs.append(job_loc.text)\n",
    "    \n",
    "    return drivr, pd.DataFrame({'Job Title':titles, 'Company':companies, 'Experience Required':exps,'Salary offered':salaries, 'Location':locs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_descrp_naukri(driver, stop = None):\n",
    "    main_window = driver.current_window_handle\n",
    "    all_windows = driver.window_handles\n",
    "\n",
    "    for window in all_windows:\n",
    "        if window != main_window:\n",
    "            driver.switch_to.window(window)\n",
    "            driver.close()\n",
    "\n",
    "    driver.switch_to.window(main_window)\n",
    "    main_window = driver.current_window_handle\n",
    "    all_windows = driver.window_handles\n",
    "\n",
    "    full_descriptions = []\n",
    "\n",
    "    for element in driver.find_elements_by_xpath(\"//div[@class='info fleft']/a\")[:stop]:\n",
    "        element.click()\n",
    "        time.sleep(2)\n",
    "        all_windows = driver.window_handles\n",
    "        driver.switch_to.window(all_windows[1])\n",
    "        try:\n",
    "            full_descriptions.append(driver.find_element_by_xpath(\"//section[@class='job-desc']\").text)\n",
    "        except:\n",
    "            full_descriptions.append(driver.find_element_by_xpath(\"//div[@class='clearboth description']\").text)\n",
    "        driver.close()\n",
    "        driver.switch_to.window(main_window)\n",
    "    return driver, full_descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_glassdoor(driver):\n",
    "    username = 'testacount573@gmail.com'\n",
    "    paswd = 'Testaccount123'\n",
    "    \n",
    "    main_window = driver.current_window_handle\n",
    "    try:\n",
    "        driver.find_element_by_class_name(\"locked-home-sign-in\").click()\n",
    "    except NoSuchElementException:\n",
    "        driver.find_element_by_class_xpath(\"//div[@class='d-flex']//button\").click()\n",
    "        \n",
    "    time.sleep(3)\n",
    "    #all_windows = driver.window_handles\n",
    "    #print(all_windows)\n",
    "    #driver.switch_to.window(all_windows[1])\n",
    "    \n",
    "    driver.find_element_by_id(\"userEmail\").send_keys(username)\n",
    "    driver.find_element_by_id(\"userPassword\").send_keys(paswd)\n",
    "    driver.find_element_by_xpath(\"//button[contains(text(),'Sign In')]\").click()\n",
    "    time.sleep(3)\n",
    "    driver.switch_to.window(main_window)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highlevel_details_glassdoor(job, location = None, salary_range=None, stop=None, use_filter=False):\n",
    "    url = 'https://www.glassdoor.co.in/index.htm'\n",
    "    drivr = get_drive_launch(url)\n",
    "    drivr = login_glassdoor(drivr)\n",
    "    drivr.find_element_by_id(\"sc.keyword\").send_keys(job)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    if ((location) and (not use_filter)):\n",
    "        drivr.find_element_by_xpath(\"//input[@data-test='search-bar-location-input']\").send_keys(Keys.CONTROL + \"a\")\n",
    "        drivr.find_element_by_xpath(\"//input[@data-test='search-bar-location-input']\").send_keys(Keys.DELETE)\n",
    "        drivr.find_element_by_xpath(\"//input[@data-test='search-bar-location-input']\").send_keys(location)\n",
    "    time.sleep(2)\n",
    "    drivr.find_element_by_xpath(\"//button[@type='submit']\").click()\n",
    "    #click to expand location filter \n",
    "    if use_filter:        \n",
    "        if location:\n",
    "            time.sleep(3)\n",
    "            \n",
    "            try:\n",
    "                drivr.find_element_by_xpath(\"//span[@class = 'ellipsis fleft' and contains(text(), \"+\"'\"+location+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "            except:\n",
    "                drivr.find_element_by_xpath(\"//div[@data-filter-id='citiesGid']//a\").click()\n",
    "                time.sleep(3)\n",
    "                drivr.find_element_by_xpath(\"//span[@class = 'ellipsis fleft' and contains(text(), \"+\"'\"+location+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "        if salary_range:\n",
    "\n",
    "            try:\n",
    "                drivr.find_element_by_xpath(\"//span[contains(text(), \"+\"'\"+salary_range+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "            except:\n",
    "                drivr.find_element_by_xpath(\"//div[@data-filter-id='salaryRange']//a\").click()\n",
    "                time.sleep(3)\n",
    "                drivr.find_element_by_xpath(\"//span[contains(text(), \"+\"'\"+salary_range+\"'\"+\")]/../preceding-sibling::i\").click()\n",
    "\n",
    "    time.sleep(5)\n",
    "    print(drivr.find_element_by_xpath(\"//h1[@data-test='jobTitle']\").text)\n",
    "    titles=[]\n",
    "    companies=[]\n",
    "    no_days_posts=[]\n",
    "    #salaries=[]\n",
    "    locs=[]\n",
    "    ratings=[]\n",
    "    \n",
    "    job_links = drivr.find_elements_by_xpath(\"//div[@class='d-flex flex-column css-x75kgh e1rrn5ka3']\")\n",
    "    \n",
    "    for job_link in job_links[:stop]:\n",
    "        try:\n",
    "            if job_link.text:\n",
    "                ratings.append(job_link.text)\n",
    "            else:\n",
    "                ratings.append('-')\n",
    "            #ratings.append(job_link.find_element_by_xpath(\"//span[@class='css-19pjha7 e1cjmv6j1']\").text)\n",
    "        except:\n",
    "            ratings.append(\"No Rating\")\n",
    "    job_titles = drivr.find_elements_by_xpath(\"//a[@data-test='job-link']/span\")\n",
    "    job_companies = drivr.find_elements_by_xpath(\"//div[@class='d-flex justify-content-between align-items-start']//a/span\")\n",
    "    job_days = drivr.find_elements_by_xpath(\"//div[@data-test='job-age']\")\n",
    "    #job_salaries = drivr.find_elements_by_xpath(\"//div[@class='info fleft']/ul/li[2]\")\n",
    "    job_locations = drivr.find_elements_by_xpath(\"//div[@class='d-flex flex-wrap css-11d3uq0 e1rrn5ka2']/span\")\n",
    "    \n",
    "    for job_title, job_company, job_day, job_loc in zip(job_titles[:stop],job_companies[:stop],job_days[:stop],job_locations[:stop]):\n",
    "        titles.append(job_title.text.strip())\n",
    "        companies.append(job_company.text.strip())\n",
    "        no_days_posts.append(job_day.text.strip())\n",
    "        #salaries.append(job_salary.text)\n",
    "        locs.append(job_loc.text.strip())\n",
    "    \n",
    "    return drivr, pd.DataFrame({'Job Title':titles, 'Company':companies, 'Job Post Age':no_days_posts, 'Location':locs, 'Ratings':ratings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_highlevel_details_flipkart(product_name, no_of_products = 100):\n",
    "    url = 'https://www.flipkart.com/'\n",
    "    driver = get_drive_launch(url)\n",
    "    driver.find_element_by_xpath(\"//button[contains(text(),'✕')]\").click()\n",
    "\n",
    "    driver.find_element_by_xpath(\"//input[@title='Search for products, brands and more']\").send_keys(product_name)\n",
    "    driver.find_element_by_xpath(\"//button[@type='submit']\").click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    product_names = []\n",
    "    product_descriptions = []\n",
    "    product_prices = []\n",
    "    product_discounts = []\n",
    "\n",
    "    while len(product_names) < no_of_products:\n",
    "        product_name_elems = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "        product_description_elems = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']/../a[not(@class='_3bPFwb')]\")\n",
    "        price_elems= driver.find_elements_by_xpath(\"//a[@class='_3bPFwb']//div[@class='_30jeq3']\")\n",
    "        price_discount_elems = driver.find_elements_by_xpath(\"//a[@class='_3bPFwb']\")\n",
    "        for prod_name_elem, product_description_elem, price_elem, price_discount_elem in zip(product_name_elems,product_description_elems,price_elems,price_discount_elems):\n",
    "            product_names.append(prod_name_elem.text.strip())\n",
    "            product_descriptions.append(product_description_elem.text.strip())\n",
    "            product_prices.append(price_elem.text.strip())\n",
    "            try:\n",
    "                discount = price_discount_elem.find_element_by_class_name(\"_3Ay6Sb\").text\n",
    "            except:\n",
    "                discount = None\n",
    "            if discount:\n",
    "                product_discounts.append(discount)\n",
    "            else:\n",
    "                product_discounts.append(\"0% off\")\n",
    "\n",
    "            if len(product_names) == no_of_products:\n",
    "                break\n",
    "        driver.find_element_by_xpath(\"//span[contains(text(),'Next')]\").click()\n",
    "        time.sleep(3)\n",
    "    dataframe = pd.DataFrame({'Product Brand': product_names, 'Product Description' : product_descriptions, 'Product Price' : product_prices,'Product Discount':product_discounts})\n",
    "    return driver, dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data. \n",
    "\n",
    "Note- All of the above steps have to be done in code. No step is to be done manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Analyst Jobs In Bangalore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience Required</th>\n",
       "      <th>Salary offered</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Assistant Manager II - Data Analyst</td>\n",
       "      <td>Flipkart Internet Private Limited</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Super India Tech Mark</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>1,25,000 - 2,25,000 PA.</td>\n",
       "      <td>Bangalore/Bengaluru(Devalapur)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>tech mahindra ltd</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>GlaxoSmithKline Pharmaceuticals Limited</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Xiaomi</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hiring For Data Analyst</td>\n",
       "      <td>Concentrix Daksh Services India Private Limited.</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>4,00,000 - 9,00,000 PA.</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Chennai, Bangalo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Cerner</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst with Marketing Analytics-Capco</td>\n",
       "      <td>Capco Technologies Pvt Ltd</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "      <td>7,00,000 - 17,00,000 PA.</td>\n",
       "      <td>Pune, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Business Data Analyst - MIS &amp; Reporting</td>\n",
       "      <td>INTERTRUST GROUP</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Mumbai, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Junior Data Analyst</td>\n",
       "      <td>Happy Marketer Private Ltd</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "      <td>3,00,000 - 7,00,000 PA.</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai (All Areas)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Job Title  \\\n",
       "0          Assistant Manager II - Data Analyst   \n",
       "1                                 Data Analyst   \n",
       "2                                 Data Analyst   \n",
       "3                                 Data Analyst   \n",
       "4                                 Data Analyst   \n",
       "5                      Hiring For Data Analyst   \n",
       "6                          Senior Data Analyst   \n",
       "7  Data Analyst with Marketing Analytics-Capco   \n",
       "8      Business Data Analyst - MIS & Reporting   \n",
       "9                          Junior Data Analyst   \n",
       "\n",
       "                                            Company Experience Required  \\\n",
       "0                 Flipkart Internet Private Limited             4-8 Yrs   \n",
       "1                             Super India Tech Mark             0-2 Yrs   \n",
       "2                                 tech mahindra ltd             4-8 Yrs   \n",
       "3           GlaxoSmithKline Pharmaceuticals Limited             2-7 Yrs   \n",
       "4                                            Xiaomi             2-6 Yrs   \n",
       "5  Concentrix Daksh Services India Private Limited.             2-7 Yrs   \n",
       "6                                            Cerner             3-5 Yrs   \n",
       "7                        Capco Technologies Pvt Ltd             4-9 Yrs   \n",
       "8                                  INTERTRUST GROUP             3-7 Yrs   \n",
       "9                        Happy Marketer Private Ltd             1-3 Yrs   \n",
       "\n",
       "             Salary offered                                           Location  \n",
       "0             Not disclosed                                Bangalore/Bengaluru  \n",
       "1   1,25,000 - 2,25,000 PA.                     Bangalore/Bengaluru(Devalapur)  \n",
       "2             Not disclosed                                Bangalore/Bengaluru  \n",
       "3             Not disclosed                                Bangalore/Bengaluru  \n",
       "4             Not disclosed                                Bangalore/Bengaluru  \n",
       "5   4,00,000 - 9,00,000 PA.  Hyderabad/Secunderabad, Pune, Chennai, Bangalo...  \n",
       "6             Not disclosed                                Bangalore/Bengaluru  \n",
       "7  7,00,000 - 17,00,000 PA.                          Pune, Bangalore/Bengaluru  \n",
       "8             Not disclosed                        Mumbai, Bangalore/Bengaluru  \n",
       "9   3,00,000 - 7,00,000 PA.            Bangalore/Bengaluru, Mumbai (All Areas)  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, dataFrame = get_highlevel_details_naukri('Data Analyst', location='Bangalore', stop=10)\n",
    "d.quit()\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note- \n",
    "1. All of the above steps have to be done in code. No step is to be done manually.\n",
    "2. Please note that you have to scrape full job description. For that you may have to open each job separately as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scientist Jobs In Bangalore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Full Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Required- Data Scientist (NLP)-Axis Bank - 6 m...</td>\n",
       "      <td>Axis Bank Limited</td>\n",
       "      <td>Kolkata, New Delhi, Hyderabad/Secunderabad, Pu...</td>\n",
       "      <td>Roles and Responsibilities - The bank generate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist - Python/ MATLAB/ Machine Learn...</td>\n",
       "      <td>Wrackle Technologies Pvt Ltd</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nData Scientist - Data Mining/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Principal Data Scientist - Machine/Deep Learni...</td>\n",
       "      <td>Fidius advisory</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nJob Description :\\n- We are l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist/ Analyst</td>\n",
       "      <td>Becton Dickinson India Pvt. Ltd</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nRoles and Responsibilities\\no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Opportunity For Data Scientist Internship - Be...</td>\n",
       "      <td>Corner Stone Solutions</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nLocation - Bangalore / Bengal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist || Data Analyst || Data science</td>\n",
       "      <td>Inspiration Manpower Consultancy Pvt. Ltd.</td>\n",
       "      <td>Navi Mumbai, Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nJob description\\nJob Summary ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Scientist, , Data Science</td>\n",
       "      <td>Visa Inc.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nWe are seeking an innovative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lead Data Scientist - Immediate Joiners are Re...</td>\n",
       "      <td>Techolution India Private Limited</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Pune, Chennai,...</td>\n",
       "      <td>Job description\\nWe are looking for qualified ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Scientist (Machine Vision solutions)</td>\n",
       "      <td>ONX Software Systems Pvt Ltd</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job description\\nSenior Data Scientist (Machin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Global Medical Data Scientist</td>\n",
       "      <td>GlaxoSmithKline Pharmaceuticals Limited</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>This is an ideal role for an experienced candi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0  Required- Data Scientist (NLP)-Axis Bank - 6 m...   \n",
       "1  Data Scientist - Python/ MATLAB/ Machine Learn...   \n",
       "2  Principal Data Scientist - Machine/Deep Learni...   \n",
       "3                            Data Scientist/ Analyst   \n",
       "4  Opportunity For Data Scientist Internship - Be...   \n",
       "5     Data Scientist || Data Analyst || Data science   \n",
       "6              Senior Data Scientist, , Data Science   \n",
       "7  Lead Data Scientist - Immediate Joiners are Re...   \n",
       "8   Senior Data Scientist (Machine Vision solutions)   \n",
       "9                      Global Medical Data Scientist   \n",
       "\n",
       "                                      Company  \\\n",
       "0                           Axis Bank Limited   \n",
       "1                Wrackle Technologies Pvt Ltd   \n",
       "2                             Fidius advisory   \n",
       "3             Becton Dickinson India Pvt. Ltd   \n",
       "4                      Corner Stone Solutions   \n",
       "5  Inspiration Manpower Consultancy Pvt. Ltd.   \n",
       "6                                   Visa Inc.   \n",
       "7           Techolution India Private Limited   \n",
       "8                ONX Software Systems Pvt Ltd   \n",
       "9     GlaxoSmithKline Pharmaceuticals Limited   \n",
       "\n",
       "                                            Location  \\\n",
       "0  Kolkata, New Delhi, Hyderabad/Secunderabad, Pu...   \n",
       "1                                Bangalore/Bengaluru   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5                   Navi Mumbai, Bangalore/Bengaluru   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7  Mumbai, Hyderabad/Secunderabad, Pune, Chennai,...   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                                    Full Description  \n",
       "0  Roles and Responsibilities - The bank generate...  \n",
       "1  Job description\\nData Scientist - Data Mining/...  \n",
       "2  Job description\\nJob Description :\\n- We are l...  \n",
       "3  Job description\\nRoles and Responsibilities\\no...  \n",
       "4  Job description\\nLocation - Bangalore / Bengal...  \n",
       "5  Job description\\nJob description\\nJob Summary ...  \n",
       "6  Job description\\nWe are seeking an innovative ...  \n",
       "7  Job description\\nWe are looking for qualified ...  \n",
       "8  Job description\\nSenior Data Scientist (Machin...  \n",
       "9  This is an ideal role for an experienced candi...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, dataFrame = get_highlevel_details_naukri('Data Scientist', location='Bangalore', stop=10)\n",
    "d, dataFrame['Full Description'] = get_full_descrp_naukri(d, 10)\n",
    "d.quit()\n",
    "dataFrame.drop(['Experience Required', 'Salary offered'], axis=1, inplace = True)\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Job description\\nJob Description :\\n- We are looking for a researcher who specializes in building personalization/recommender systems algorithm (ML APIs) for the applications mentioned above and work with our engineers to deploy them as scale.\\nWhat you will do :\\n- Apply your research expertise to build our ML-driven recommender system products, help us develop new solutions and unlock new directions, as well as analyse and optimise the systems we already. \\n- You'll work closely with product teams and mentor them on best practices for modern ML, and keep the wider team informed on the state-of-the-art. In addition, you will be in a strategic position to influence future roadmaps for recommender system products.\\n- Collaborate with a cross-functional agile team spanning user research, design, data science, product management, and engineering to build new product features that advance our mission to connect artists and fans in personalized and relevant ways.\\n- Prototype new approaches and production-ize solutions at scale for our hundreds of thousands of active users. Help drive optimization, testing, and tools to improve quality.\\nRequirements :\\n- Master, Post-graduate or Ph.D. in computer science, machine learning, information retrieval, recommendation systems, natural language processing, statistics, math, engineering, operations research, or another quantitative discipline; or equivalent work experience.\\n- Good theoretical grounding in core machine learning concepts and techniques.\\n- Ability to perform comprehensive literature reviews and provide critical feedback on state-of-the-art solutions and how they may fit different operating constraints.\\n- Experience with a number of ML techniques and frameworks, e.g. Natural Language Processing, Recommender Systems, sampling, linear regression, decision trees, SVMs, deep neural networks, etc.\\n- Familiarity with one or more Deep learning software frameworks such as Tensorflow, PyTorch.\\nRoleAnalytics Manager\\nIndustry TypeIT-Software, Software Services\\nFunctional AreaAnalytics & Business Intelligence\\nEmployment TypeFull Time, Permanent\\nRole CategoryAnalytics & BI\\nEducation\\nUG :Any Graduate in Any Specialization\\nPG :M.Tech in Computers, MS/M.Sc(Science) in Computers\\nKey Skills\\nTensorflowNLPPyTorchdata scienceproduct managementdesignDeep Learning\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame.loc[2, 'Full Description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You have to use the location and salary filter.\n",
    "2. You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "3. You have to scrape the job-title, job-location, company_name, experience_required.\n",
    "\n",
    "1. The location filter to be used is “Delhi/NCR”\n",
    "2. The salary filter to be used is “3-6” lakhs\n",
    "\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field .WEB SCRAPING ASSIGNMENT-2.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note- All of the above steps have to be done in code. No step is to be done \n",
    "manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scientist Jobs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience Required</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Developer - Data Science</td>\n",
       "      <td>ICL Systems India Private Limited</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "      <td>Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Required- Data Scientist (NLP)-Axis Bank - 6 m...</td>\n",
       "      <td>Axis Bank Limited</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "      <td>Kolkata, New Delhi, Hyderabad/Secunderabad, Pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist/Data Analyst - Python/Machine L...</td>\n",
       "      <td>Change leaders</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Mumbai, Ghaziabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business Consultant Data Scientist</td>\n",
       "      <td>StriveX Consulting Pvt. Ltd</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "      <td>Hyderabad/Secunderabad, Chennai, Bangalore/Ben...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Amity University</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "      <td>Ghaziabad, Faridabad, Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist - Python &amp; Machine Learning</td>\n",
       "      <td>FUTURES AND CAREERS</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Bangalore/Bengal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist - Python &amp; Machine Learning</td>\n",
       "      <td>FUTURES AND CAREERS</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Hyderabad/Secunderabad, Bangalore/Bengaluru, D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist - Machine Learning/ NLP</td>\n",
       "      <td>TalPro</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hiring For Data Scientist</td>\n",
       "      <td>Max Bupa Health Insurance Company Limited</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "      <td>Gurgaon/Gurugram, Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist - Python / Machine Learning / T...</td>\n",
       "      <td>FUTURES AND CAREERS</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Hyderabad/Secunderabad, Bangalore/Bengaluru, D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0                           Developer - Data Science   \n",
       "1  Required- Data Scientist (NLP)-Axis Bank - 6 m...   \n",
       "2  Data Scientist/Data Analyst - Python/Machine L...   \n",
       "3                 Business Consultant Data Scientist   \n",
       "4                                     Data Scientist   \n",
       "5         Data Scientist - Python & Machine Learning   \n",
       "6         Data Scientist - Python & Machine Learning   \n",
       "7             Data Scientist - Machine Learning/ NLP   \n",
       "8                          Hiring For Data Scientist   \n",
       "9  Data Scientist - Python / Machine Learning / T...   \n",
       "\n",
       "                                     Company Experience Required  \\\n",
       "0          ICL Systems India Private Limited             3-5 Yrs   \n",
       "1                          Axis Bank Limited             4-9 Yrs   \n",
       "2                             Change leaders            5-10 Yrs   \n",
       "3                StriveX Consulting Pvt. Ltd             2-4 Yrs   \n",
       "4                           Amity University             6-8 Yrs   \n",
       "5                        FUTURES AND CAREERS             2-7 Yrs   \n",
       "6                        FUTURES AND CAREERS             2-7 Yrs   \n",
       "7                                     TalPro             2-6 Yrs   \n",
       "8  Max Bupa Health Insurance Company Limited             1-6 Yrs   \n",
       "9                        FUTURES AND CAREERS             3-8 Yrs   \n",
       "\n",
       "                                            Location  \n",
       "0                                        Delhi / NCR  \n",
       "1  Kolkata, New Delhi, Hyderabad/Secunderabad, Pu...  \n",
       "2                                  Mumbai, Ghaziabad  \n",
       "3  Hyderabad/Secunderabad, Chennai, Bangalore/Ben...  \n",
       "4                  Ghaziabad, Faridabad, Delhi / NCR  \n",
       "5  Hyderabad/Secunderabad, Pune, Bangalore/Bengal...  \n",
       "6  Hyderabad/Secunderabad, Bangalore/Bengaluru, D...  \n",
       "7                                   Gurgaon/Gurugram  \n",
       "8                      Gurgaon/Gurugram, Delhi / NCR  \n",
       "9  Hyderabad/Secunderabad, Bangalore/Bengaluru, D...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver,dataFrame=get_highlevel_details_naukri('Data Scientist', location = 'Delhi / NCR', salary_range='3-6 Lakhs', stop=10, use_filter=True)\n",
    "driver.quit()\n",
    "dataFrame.drop(['Salary offered'],axis=1,inplace=True)\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "2. Enter “Data Scientist” in “Job Title,Keyword,Company” field and enter “Noida” in “location” field.\n",
    "3. Then click the search button. You will land up in the below page:\n",
    "4. Then scrape the data for the first 10 jobs results you get in the above shown page.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note- All of the above steps have to be done in code. No step is to be done\n",
    "manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scientist Jobs in Noida\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Job Post Age</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Innovaccer Analytics Private Limited</td>\n",
       "      <td>8d</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Asquero</td>\n",
       "      <td>24h</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unyscape Infocom Pvt. Ltd</td>\n",
       "      <td>25d</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Biz2Credit Inc</td>\n",
       "      <td>30d+</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Techlive</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Salasar New Age Technologies</td>\n",
       "      <td>30d+</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Adobe</td>\n",
       "      <td>5d</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SysQuo Innovation Private Limited</td>\n",
       "      <td>9d</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Salasar New Age Technologies</td>\n",
       "      <td>30d+</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IBM</td>\n",
       "      <td>5d</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Company Job Post Age Ratings\n",
       "0  Innovaccer Analytics Private Limited           8d     3.5\n",
       "1                               Asquero          24h     5.0\n",
       "2             Unyscape Infocom Pvt. Ltd          25d     4.1\n",
       "3                        Biz2Credit Inc         30d+     3.8\n",
       "4                              Techlive                    -\n",
       "5          Salasar New Age Technologies         30d+       -\n",
       "6                                 Adobe           5d     4.4\n",
       "7     SysQuo Innovation Private Limited           9d     3.0\n",
       "8          Salasar New Age Technologies         30d+       -\n",
       "9                                   IBM           5d     4.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver, dataframe = get_highlevel_details_glassdoor('Data Scientist','Noida',stop=10)\n",
    "driver.quit()\n",
    "dataframe=dataframe[['Company', 'Job Post Age', 'Ratings']]\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location. You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above task will be, done as shown in the below steps:\n",
    "1. first get the webpage https://www.glassdoor.co.in/Salaries/index.htm\n",
    "2. Enter “Data Scientist” in Job title field and “Noida” in location field.\n",
    "3. Click the search button.\n",
    "4. After that you will land on the below page You have to scrape whole data from this webpage\n",
    "5. Scrape data for first 10 companies. Scrape the min salary, max salary, company name, Average salary and rating of the company.\n",
    "6. Store the data in a dataframe.\n",
    "\n",
    "Note that all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'two', 't', 'h', 'r', 'e', 'e']\n"
     ]
    }
   ],
   "source": [
    "x = ['one','two']\n",
    "x.extend('three')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_glassdoor_salary(driver):\n",
    "    username = 'testacount573@gmail.com'\n",
    "    paswd = 'Testaccount123'\n",
    "    \n",
    "    main_window = driver.current_window_handle\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//li[@class='sign-in']/a\").click()\n",
    "    except NoSuchElementException:\n",
    "        #driver.find_element_by_class_xpath(\"//div[@class='d-flex']//button\").click()\n",
    "        pass\n",
    "        \n",
    "    time.sleep(3)\n",
    "    #all_windows = driver.window_handles\n",
    "    #print(all_windows)\n",
    "    #driver.switch_to.window(all_windows[1])\n",
    "    \n",
    "    driver.find_element_by_id(\"userEmail\").send_keys(username)\n",
    "    driver.find_element_by_id(\"userPassword\").send_keys(paswd)\n",
    "    driver.find_element_by_xpath(\"//button[contains(text(),'Sign In')]\").click()\n",
    "    time.sleep(3)\n",
    "    driver.switch_to.window(main_window)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_salary_details_glassdoor(job_title, location, stop=None):\n",
    "    url = 'https://www.glassdoor.co.in/Salaries/index.htm'\n",
    "    drivr = get_drive_launch(url)\n",
    "    drivr = login_glassdoor_salary(drivr)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    drivr.find_element_by_id(\"KeywordSearch\").send_keys(job_title)\n",
    "    drivr.find_element_by_id(\"LocationSearch\").send_keys(Keys.CONTROL + \"a\")\n",
    "    drivr.find_element_by_id(\"LocationSearch\").send_keys(Keys.DELETE)\n",
    "    drivr.find_element_by_id(\"LocationSearch\").send_keys(location)\n",
    "    drivr.find_element_by_id(\"HeroSearchButton\").click()\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6 : Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount %\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape the data you have to go through following steps:\n",
    "1. Go to flipkart webpage by url https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon\n",
    "3. after that you will reach to a webpage having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "4. after scraping data from the first page, go to the “Next” Button at the bottom of the page , then click on it\n",
    "5. Now scrape data from this page as usual\n",
    "6. repeat this until you get data for 100 sunglasses.\n",
    "\n",
    "Note that all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Product Price</th>\n",
       "      <th>Product Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Round Sunglasses (55)</td>\n",
       "      <td>₹449</td>\n",
       "      <td>82% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Butterfly Sunglasses (55)</td>\n",
       "      <td>₹209</td>\n",
       "      <td>86% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹758</td>\n",
       "      <td>15% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹569</td>\n",
       "      <td>28% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Mirrored, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ALEYBEE</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹189</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>GANSTA</td>\n",
       "      <td>UV Protection, Night Vision, Riding Glasses Av...</td>\n",
       "      <td>₹295</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Round Sunglasses (Free...</td>\n",
       "      <td>₹399</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>hipe</td>\n",
       "      <td>UV Protection Round Sunglasses (Free Size)</td>\n",
       "      <td>₹228</td>\n",
       "      <td>82% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NuVew</td>\n",
       "      <td>UV Protection, Mirrored Clubmaster Sunglasses ...</td>\n",
       "      <td>₹167</td>\n",
       "      <td>77% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Product Brand                                Product Description  \\\n",
       "0           PIRASO                UV Protection Round Sunglasses (55)   \n",
       "1           PIRASO            UV Protection Butterfly Sunglasses (55)   \n",
       "2         Fastrack      UV Protection Wayfarer Sunglasses (Free Size)   \n",
       "3         Fastrack   UV Protection Rectangular Sunglasses (Free Size)   \n",
       "4         Fastrack  Mirrored, UV Protection Wayfarer Sunglasses (F...   \n",
       "..             ...                                                ...   \n",
       "95         ALEYBEE                UV Protection Round Sunglasses (54)   \n",
       "96          GANSTA  UV Protection, Night Vision, Riding Glasses Av...   \n",
       "97  ROZZETTA CRAFT  UV Protection, Gradient Round Sunglasses (Free...   \n",
       "98            hipe         UV Protection Round Sunglasses (Free Size)   \n",
       "99           NuVew  UV Protection, Mirrored Clubmaster Sunglasses ...   \n",
       "\n",
       "   Product Price Product Discount  \n",
       "0           ₹449          82% off  \n",
       "1           ₹209          86% off  \n",
       "2           ₹758          15% off  \n",
       "3           ₹569          28% off  \n",
       "4           ₹499          50% off  \n",
       "..           ...              ...  \n",
       "95          ₹189          87% off  \n",
       "96          ₹295          85% off  \n",
       "97          ₹399          80% off  \n",
       "98          ₹228          82% off  \n",
       "99          ₹167          77% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, dataFrame = get_product_highlevel_details_flipkart('sunglasses', 100)\n",
    "d.quit()\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes\u0002earpods-power\u0002adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you will open the above link you will reach to the below shown webpage.\n",
    "\n",
    "As shown in the above page you have to scrape the tick marked attributes.\n",
    "These are \n",
    "1. Rating \n",
    "2. Review_summary \n",
    "3. Full review\n",
    "\n",
    "You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews_flipkart(product_url, no_of_reviews):\n",
    "    drivr = get_drive_launch(product_url)\n",
    "    product_reviews_url = drivr.find_element_by_xpath(\"//div[@class='_3UAT2v _16PBlm']/..\").get_attribute('href')\n",
    "    drivr.quit()\n",
    "    drivr = get_drive_launch(product_reviews_url)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    ratings = []\n",
    "    review_summaries = []\n",
    "    full_reviews = []\n",
    "\n",
    "    while len(ratings) < no_of_reviews:\n",
    "        rating_elems = drivr.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "        review_summary_elems = drivr.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "        full_review_elems= drivr.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "\n",
    "        for rating_elem,review_summary_elem,full_review_elem in zip(rating_elems,review_summary_elems,full_review_elems):\n",
    "            ratings.append(rating_elem.text.strip())\n",
    "            review_summaries.append(review_summary_elem.text.strip())\n",
    "            full_reviews.append(full_review_elem.text.strip())\n",
    "            if len(ratings) == no_of_reviews:\n",
    "                break\n",
    "        drivr.find_element_by_xpath(\"//span[contains(text(),'Next')]\").click()\n",
    "        time.sleep(3)\n",
    "    dataframe = pd.DataFrame({'Rating': ratings, 'Review Summary' : review_summaries, 'Full Review' : full_reviews})\n",
    "    return drivr, dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Summary</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the Money\\n\\nThe iPhone 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.\\n\\nI’m am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>iphone 11 is a very good phone to buy only if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>Excellent camera and display touching very nic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Just wow!</td>\n",
       "      <td>Great iphone.\\nI am writing this review after ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Wonderful</td>\n",
       "      <td>Superfast delivery by Flipkart. Thanks.\\n\\n1. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>4</td>\n",
       "      <td>Good quality product</td>\n",
       "      <td>Awesome camera, smooth and fast UI, display is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Must buy!</td>\n",
       "      <td>I rate this product 5* as it has got amazing u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating        Review Summary  \\\n",
       "0       5             Brilliant   \n",
       "1       5      Perfect product!   \n",
       "2       5     Worth every penny   \n",
       "3       5         Great product   \n",
       "4       5    Highly recommended   \n",
       "..    ...                   ...   \n",
       "95      5             Brilliant   \n",
       "96      5             Just wow!   \n",
       "97      5             Wonderful   \n",
       "98      4  Good quality product   \n",
       "99      5             Must buy!   \n",
       "\n",
       "                                          Full Review  \n",
       "0   The Best Phone for the Money\\n\\nThe iPhone 11 ...  \n",
       "1   Amazing phone with great cameras and better ba...  \n",
       "2   Previously I was using one plus 3t it was a gr...  \n",
       "3   Amazing Powerful and Durable Gadget.\\n\\nI’m am...  \n",
       "4   iphone 11 is a very good phone to buy only if ...  \n",
       "..                                                ...  \n",
       "95  Excellent camera and display touching very nic...  \n",
       "96  Great iphone.\\nI am writing this review after ...  \n",
       "97  Superfast delivery by Flipkart. Thanks.\\n\\n1. ...  \n",
       "98  Awesome camera, smooth and fast UI, display is...  \n",
       "99  I rate this product 5* as it has got amazing u...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_url = 'https://www.flipkart.com/apple-iphone-11-black-64-gb-includes%02earpods-power%02adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace'\n",
    "d, DataFrame = get_reviews_flipkart(product_url, 100)\n",
    "d.quit()\n",
    "DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to scrape 4 attributes of each sneaker :\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. discount %\n",
    "\n",
    "As shown in the below image, you have to scrape the tick marked attributes\n",
    "\n",
    "Also note that all the steps required during scraping should be done through code \n",
    "only and not manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Product Price</th>\n",
       "      <th>Product Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Perfect &amp; Affordable Combo Pack of 02 Pairs Sn...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>72% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Combo Pack of 4 Casual Sneakers With Sneakers ...</td>\n",
       "      <td>₹474</td>\n",
       "      <td>76% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Extoes</td>\n",
       "      <td>Modern Trendy Shoes Combo pack of 4 Sneakers F...</td>\n",
       "      <td>₹798</td>\n",
       "      <td>46% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Robbie jones</td>\n",
       "      <td>Casual Sneakers Shoes For Men Sneakers For Men</td>\n",
       "      <td>₹379</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aadi</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹299</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Axter</td>\n",
       "      <td>Combo Pack of 2 Casual Loafer Sneakers Shoes S...</td>\n",
       "      <td>₹299</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Onivixo</td>\n",
       "      <td>Men's White Suede Leather Lace-Up High Ankle C...</td>\n",
       "      <td>₹549</td>\n",
       "      <td>54% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>SPARX</td>\n",
       "      <td>SM-482 Sneakers For Men</td>\n",
       "      <td>₹965</td>\n",
       "      <td>28% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>MOU</td>\n",
       "      <td>Zar Check Sneakers Sneakers For Men</td>\n",
       "      <td>₹523</td>\n",
       "      <td>47% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Jokatoo</td>\n",
       "      <td>casual sneaker shoes and partywear shoes Casua...</td>\n",
       "      <td>₹398</td>\n",
       "      <td>73% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Product Brand                                Product Description  \\\n",
       "0         Chevit  Perfect & Affordable Combo Pack of 02 Pairs Sn...   \n",
       "1         Chevit  Combo Pack of 4 Casual Sneakers With Sneakers ...   \n",
       "2         Extoes  Modern Trendy Shoes Combo pack of 4 Sneakers F...   \n",
       "3   Robbie jones     Casual Sneakers Shoes For Men Sneakers For Men   \n",
       "4           aadi                                   Sneakers For Men   \n",
       "..           ...                                                ...   \n",
       "95         Axter  Combo Pack of 2 Casual Loafer Sneakers Shoes S...   \n",
       "96       Onivixo  Men's White Suede Leather Lace-Up High Ankle C...   \n",
       "97         SPARX                            SM-482 Sneakers For Men   \n",
       "98           MOU                Zar Check Sneakers Sneakers For Men   \n",
       "99       Jokatoo  casual sneaker shoes and partywear shoes Casua...   \n",
       "\n",
       "   Product Price Product Discount  \n",
       "0           ₹499          72% off  \n",
       "1           ₹474          76% off  \n",
       "2           ₹798          46% off  \n",
       "3           ₹379          62% off  \n",
       "4           ₹299          70% off  \n",
       "..           ...              ...  \n",
       "95          ₹299          70% off  \n",
       "96          ₹549          54% off  \n",
       "97          ₹965          28% off  \n",
       "98          ₹523          47% off  \n",
       "99          ₹398          73% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, dataFrame = get_product_highlevel_details_flipkart('sneakers', 100)\n",
    "d.quit()\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
